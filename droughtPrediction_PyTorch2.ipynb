{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drought Prediction\n",
    "## Load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, classification_report,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, roc_curve, auc, cohen_kappa_score)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule, NearMiss\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "\n",
    "with open('data/Xy_trainTest.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model\n",
    "class DroughtNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the DroughtNet model.\n",
    "\n",
    "        Parameters:\n",
    "            input_dim (int): The number of input features.\n",
    "            hidden_dim (int): The number of neurons in the hidden layers.\n",
    "            output_dim (int): The number of output classes.\n",
    "        \"\"\"\n",
    "        super(DroughtNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)     # First fully connected layer\n",
    "        self.relu = nn.ReLU()                           # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)    # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)    # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the model.\n",
    "        \"\"\"\n",
    "        out = self.fc1(x)       # Apply first fully connected layer\n",
    "        out = self.relu(out)    # Apply ReLU activation\n",
    "        out = self.fc2(out)     # Apply second fully connected layer\n",
    "        out = self.relu(out)    # Apply ReLU activation\n",
    "        out = self.fc3(out)     # Apply output layer\n",
    "        return out\n",
    "\n",
    "# Complex Model\n",
    "class DroughtNetComplex(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the DroughtNetComplex model.\n",
    "\n",
    "        Parameters:\n",
    "            input_dim (int): The number of input features.\n",
    "            hidden_dims (list of int): A list where each element specifies the number of neurons in that hidden layer.\n",
    "            output_dim (int): The number of output classes.\n",
    "        \"\"\"\n",
    "        super(DroughtNetComplex, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])                                                                 # Input layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims)-1)])    # Hidden layers\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)                                                              # Output layer\n",
    "        self.relu = nn.ReLU()                                                                                                   # ReLU activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the model. \n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the model.\n",
    "        \"\"\"\n",
    "        out = self.relu(self.input_layer(x))    # Apply input layer and ReLU activation\n",
    "        for layer in self.hidden_layers:\n",
    "            out = self.relu(layer(out))         # Apply each hidden layer and ReLU activation\n",
    "        out = self.output_layer(out)            # Apply output layer\n",
    "        return out\n",
    "\n",
    "# Train Model\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, patience=3):\n",
    "    \"\"\"\n",
    "    Train the given model. Early stopping based on validation loss.\n",
    "\n",
    "    Parameters:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        test_loader (DataLoader): DataLoader for the test/validation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (optim.Optimizer): Optimizer for the model.\n",
    "        num_epochs (int): Number of epochs to train the model.\n",
    "        patience (int): Number of epochs to wait if validation loss stops decreasing.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Best model based on validation loss.\n",
    "        dict: A dictionary containing the training history (loss and accuracy for training and validation).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables for early stopping\n",
    "    best_val_loss = float('inf')# Set initial best validation loss to infinity\n",
    "    current_patience = 0        # Initialize patience counter\n",
    "    best_model = None           # Initialize variable to store the best model\n",
    "    best_train_history = None   # Initialize variable to store the training history of the best model\n",
    "\n",
    "    train_history = {'loss': [], 'accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to training mode, Initialize variables\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()                       # Zero the parameter gradients\n",
    "            outputs = model(X_batch)                    # Forward pass\n",
    "            loss = criterion(outputs, y_batch)          # Compute loss\n",
    "            loss.backward()                             # Backward pass\n",
    "            optimizer.step()                            # Optimize the weights\n",
    "            running_loss += loss.item() * X_batch.size(0)  # Accumulate running loss\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)            # Get predictions\n",
    "            correct += (predicted == y_batch).sum().item()  # Count correct predictions\n",
    "            total += y_batch.size(0)                        # Count total samples\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)   # Compute epoch loss\n",
    "        epoch_accuracy = correct / total                        # Compute epoch accuracy\n",
    "        train_history['loss'].append(epoch_loss)                # Record training loss\n",
    "        train_history['accuracy'].append(epoch_accuracy)        # Record training accuracy\n",
    "        \n",
    "        # Set model to Eval mode, Initialize variables\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in test_loader:\n",
    "                outputs = model(X_val)                                  # Forward pass\n",
    "                val_loss = criterion(outputs, y_val)                    # Compute validation loss\n",
    "                val_running_loss += val_loss.item() * X_val.size(0)     # Accumulate validation running loss\n",
    "                \n",
    "                _, val_predicted = torch.max(outputs, 1)                # Get validation predictions\n",
    "                val_correct += (val_predicted == y_val).sum().item()    # Count correct validation predictions\n",
    "                val_total += y_val.size(0)                              # Count total validation samples\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(test_loader.dataset)    # Compute validation epoch loss\n",
    "        val_epoch_accuracy = val_correct / val_total                    # Compute validation epoch accuracy\n",
    "        train_history['validation_loss'].append(val_epoch_loss)         # Record validation loss\n",
    "        train_history['validation_accuracy'].append(val_epoch_accuracy) # Record validation accuracy\n",
    "        \n",
    "        # Print statistics per epoch \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_epoch_loss < best_val_loss:\n",
    "            best_val_loss = val_epoch_loss\n",
    "            current_patience = 0  # Reset patience counter\n",
    "            best_model = model  # Update best model\n",
    "            best_train_history = train_history  # Update best training history\n",
    "        else:\n",
    "            current_patience += 1  # Increment patience counter\n",
    "\n",
    "            if current_patience >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1} due to lack of improvement.')\n",
    "                return best_model, best_train_history  # Return the best model and its training history\n",
    "    return best_model, best_train_history  # Return the best model and its training history\n",
    "\n",
    "# Evaluate Model\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the given model.\n",
    "    \n",
    "    Parameters:\n",
    "        model (nn.Module): The neural network model to evaluate.\n",
    "        test_loader (DataLoader): DataLoader for the test/validation data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model to Eval mode, Initialize variables\n",
    "    model.eval()\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            outputs = model(X_batch)            # Forward pass\n",
    "            _, y_pred = torch.max(outputs, 1)   # Get predictions\n",
    "            y_pred_list.append(y_pred.numpy())  # Store predictions\n",
    "\n",
    "    y_pred = np.concatenate(y_pred_list)                                                # Concatenate all predictions\n",
    "    y_test_numpy = y_test.to_numpy() if not isinstance(y_test, np.ndarray) else y_test  # Ensure y_test is a numpy array\n",
    "    y_pred_flat = y_pred.flatten()                                                      # Flatten predictions\n",
    "\n",
    "    accuracy = accuracy_score(y_test_numpy, y_pred_flat)        # Compute accuracy\n",
    "    report = classification_report(y_test_numpy, y_pred_flat)   # Generate classification report\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]                # Input dimensions for the model, number of features in data\n",
    "hidden_dim = 128                            # Number of neurons in the hidden layer    \n",
    "output_dim = len(set(y_train.tolist()))     # Output dimensions for the model\n",
    "\n",
    "# Instantiate the model\n",
    "model = DroughtNet(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()                       # Define the loss function to be CrossEntropyLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)    # Define the optimizer to be Adam, Learning rate to 0.001\n",
    "\n",
    "# Train the base model and evaluate it\n",
    "best_model, train_history = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=3)   # Train the model\n",
    "evaluate_model(best_model, test_loader)                                                                  # Evaluate the model\n",
    "\n",
    "# Save the trained base model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),  \n",
    "    'optimizer_state_dict': optimizer.state_dict(),  \n",
    "    'train_history': train_history,  \n",
    "}, 'saved_model.pth') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model_complex \u001b[38;5;241m=\u001b[39m DroughtNetComplex(input_dim, hidden_dims, output_dim)\n\u001b[0;32m      4\u001b[0m optimizer_complex \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_complex\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m train_history_complex \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_complex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_complex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m evaluate_model(model_complex, test_loader)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the trained complex model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     41\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     42\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     45\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define hidden dimensions and initialize the complex model\n",
    "hidden_dims = [128, 64, 32]\n",
    "model_complex = DroughtNetComplex(input_dim, hidden_dims, output_dim)   # Initialize the complex model\n",
    "optimizer_complex = optim.Adam(model_complex.parameters(), lr=0.001)    # Define same optimizer for complex model\n",
    "\n",
    "# Train/Evaluate the complex model\n",
    "best_model_complex, train_history_complex = train_model(model_complex, train_loader, test_loader, criterion, optimizer_complex, num_epochs=3)\n",
    "evaluate_model(model_complex, test_loader)\n",
    "\n",
    "# Save the trained complex model\n",
    "torch.save({\n",
    "    'model_state_dict': model_complex.state_dict(),  \n",
    "    'optimizer_state_dict': optimizer_complex.state_dict(), \n",
    "    'train_history': train_history_complex, \n",
    "}, 'saved_model2.pth')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search for Best Hidden Layer Size/Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hidden_layer_choices(num_choices, min_layers, max_layers, min_neurons, max_neurons):\n",
    "    \"\"\"\n",
    "    Generate a list of hidden layer configurations with random number of layers and neurons.\n",
    "\n",
    "    Args:\n",
    "        - num_choices (int): Number of different hidden layer configurations to generate.\n",
    "        - min_layers (int): Minimum number of hidden layers.\n",
    "        - max_layers (int): Maximum number of hidden layers.\n",
    "        - min_neurons (int): Minimum number of neurons in each hidden layer.\n",
    "        - max_neurons (int): Maximum number of neurons in each hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        - hidden_layer_choices (list): List of hidden layer configurations.\n",
    "    \"\"\"\n",
    "    hidden_layer_choices = []  # Initialize an empty list to store the hidden layer configurations\n",
    "    \n",
    "    # For number of hidden layer configurations\n",
    "    for _ in range(num_choices):\n",
    "        num_layers = random.randint(min_layers, max_layers)                             # Randomly choose the number of layers\n",
    "        layers = [random.randint(min_neurons, max_neurons) for _ in range(num_layers)]  # Randomly choose neurons for each layer\n",
    "        hidden_layer_choices.append(layers)                                             # Append the configuration to the list\n",
    "    \n",
    "    return hidden_layer_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter random search space: [[858, 939], [818, 195, 47, 788], [614, 142, 638], [179, 145, 356], [111, 201], [533, 614], [855, 468], [490], [632, 483, 143], [933, 667], [458, 414, 515], [731], [771, 638], [870, 853, 1005], [453, 490], [410, 504, 523, 536], [208, 31, 221, 277, 441], [768, 22, 808], [299, 95, 462, 805], [275, 428], [280, 115, 937, 670, 691], [537], [37, 720, 836, 387], [691, 154, 394, 997], [280, 15], [917, 62, 412], [780], [11, 700, 821, 268, 353], [252, 705, 20, 499], [650], [728], [36, 717, 592, 116, 872], [619, 905, 148, 791, 649], [642], [30, 1021, 185], [438, 516, 1016, 338, 131], [288, 150, 9, 283], [798, 967, 393, 924, 17], [882, 350, 971, 221, 386], [342, 712, 291, 946], [295, 392, 229, 791], [886, 705, 669], [861, 562, 178, 138], [509, 493, 309], [836, 623], [583, 222, 806, 756], [470, 329], [556, 11], [565, 492, 897], [163], [923, 551, 1016, 348, 179], [55], [367], [1007, 702], [363, 191, 753, 834], [880, 696, 715, 842], [500, 747, 255, 634], [428, 467, 142], [703, 823, 484, 796], [583, 835], [412, 205, 836, 416, 900], [864, 520], [303, 883, 139], [877, 492], [140], [353], [990, 484], [60, 651, 1021], [999, 445, 416, 987, 773], [554, 929], [452, 291, 810, 678, 142], [144, 471, 587, 690], [969], [654, 323, 429, 715, 943], [774, 928, 106, 355], [150, 41, 203, 359, 426], [842, 613], [175, 384, 924, 134], [670, 644, 100, 404, 21], [513], [175, 997, 682, 639, 20], [357, 711], [773, 363, 672, 559, 348], [366, 614, 407, 423], [84, 967, 836], [978, 291, 352, 397], [210], [278, 888, 429], [662, 1013, 809, 812], [141, 700, 110, 349, 45], [284, 158, 617, 762], [925, 66, 758], [417, 527, 446, 990, 662], [518, 776, 925, 713, 802], [928], [592, 436, 800], [831, 459, 988, 202], [941, 619, 529, 673, 839], [135, 170, 170], [410, 734, 16]]\n"
     ]
    }
   ],
   "source": [
    "num_choices = 100   # Number of different hidden layer configurations to generate\n",
    "min_layers = 1      # Minimum number of hidden layers\n",
    "max_layers = 5      # Maximum number of hidden layers\n",
    "min_neurons = 8     # Minimum number of neurons in each hidden layer\n",
    "max_neurons = 1024  # Maximum number of neurons in each hidden layer\n",
    "\n",
    "# Generate hidden layer choices\n",
    "hidden_layer_choices = generate_hidden_layer_choices(num_choices, min_layers, max_layers, min_neurons, max_neurons)\n",
    "\n",
    "# Print the generated hyperparameter search space\n",
    "print(f'Hyperparameter random search space: {hidden_layer_choices}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m DroughtNetComplex(input_dim, hidden_dims, output_dim)\n\u001b[0;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m train_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m train_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_accuracy \u001b[38;5;241m>\u001b[39m best_val_accuracy:\n",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     41\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     42\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     45\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize variables for search\n",
    "num_searches = 10\n",
    "best_model = None\n",
    "best_val_accuracy = 0\n",
    "best_hidden_dims = None\n",
    "\n",
    "# Random search for num_searches\n",
    "for i in range(num_searches):\n",
    "    print(f\"Random Search {i+1}/{num_searches}\")                    # Print the current random search iteration\n",
    "    hidden_dims = random.choice(hidden_layer_choices)               # Choose random hidden layer dimensions\n",
    "    model = DroughtNetComplex(input_dim, hidden_dims, output_dim)   # Initialize the complex model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)            # Initialize the optimizer\n",
    "    \n",
    "    # Train the model with early stopping and get the best model and its training history\n",
    "    best_model, train_history = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=3)\n",
    "    val_accuracy = train_history['validation_accuracy'][-1]\n",
    "    \n",
    "    # Update the best model and accuracy if current validation accuracy is better\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "        best_hidden_dims = hidden_dims\n",
    "\n",
    "# Print the best validation accuracy and corresponding hidden dimensions\n",
    "print(f\"Best validation accuracy: {best_val_accuracy}\")\n",
    "print(f\"Best hidden dimensions: {best_hidden_dims}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best model with the best hidden dimensions\n",
    "model = DroughtNetComplex(input_dim, best_hidden_dims, output_dim)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "# Train/Evaluate the best model with more epochs and early stopping\n",
    "best_model, train_history = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs=100, patience=3) \n",
    "evaluate_model(model, test_loader)                                                                  \n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_history': train_history,\n",
    "}, 'saved_best_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
