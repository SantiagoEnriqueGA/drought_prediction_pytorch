{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Scikit-learn libraries for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, ParameterGrid\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay, classification_report,\n",
    "                             accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, roc_curve, auc, cohen_kappa_score)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Imbalanced-learn libraries for handling imbalanced datasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule, NearMiss\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "from torchviz import make_dot\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Scipy library for statistical functions\n",
    "from scipy.stats import uniform\n",
    "# Base classes for custom estimators in scikit-learn\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device, use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "\n",
    "# Load training and testing data from a pickle file\n",
    "with open('data/Xy_trainTest.pkl', 'rb') as f:\n",
    "    # Unpickle the data into training and testing datasets\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation split ratio\n",
    "val_split_ratio = 0.2\n",
    "val_size = int(len(train_dataset) * val_split_ratio)\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network Classes/Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class: DroughtClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "class DroughtClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network classifier for drought prediction.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input features.\n",
    "        hidden_sizes (list of int): A list containing the sizes of the hidden layers.\n",
    "        output_size (int): The number of output classes.\n",
    "        dropout_prob (float, optional): The probability of an element to be zeroed in dropout. Default is 0.5.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): A list of linear layers.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.5):\n",
    "        super(DroughtClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        # Apply each layer followed by ReLU activation and dropout, except the last layer\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.dropout(F.relu(layer(x)))\n",
    "        # Apply the last layer without activation or dropout\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network classifier for drought prediction.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input features.\n",
    "        hidden_sizes (list of int): A list containing the sizes of the hidden layers.\n",
    "        output_size (int): The number of output classes.\n",
    "        dropout_prob (float, optional): The probability of an element to be zeroed in dropout. Default is 0.5.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): A list of linear layers.\n",
    "        activations (nn.ModuleList): A list of activation functions.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.5):\n",
    "        super(DroughtClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activations = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        self.activations.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.activations.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        for layer, activation in zip(self.layers[:-1], self.activations):\n",
    "            x = self.dropout(activation(layer(x)))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class: EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after a given patience.\n",
    "\n",
    "    Args:\n",
    "        patience (int, optional): How long to wait after last time validation loss improved. Default is 5.\n",
    "        delta (float, optional): Minimum change in the monitored quantity to qualify as an improvement. Default is 0.\n",
    "\n",
    "    Attributes:\n",
    "        patience (int): How long to wait after last time validation loss improved.\n",
    "        delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        best_loss (float): Best recorded validation loss.\n",
    "        counter (int): Counter for how many epochs have passed since the last improvement.\n",
    "        early_stop (bool): Whether early stopping is triggered.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss has improved and updates the counter and early stop flag accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): The current validation loss.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            # If the validation loss has improved (by more than delta), reset the counter\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # If the validation loss has not improved, increment the counter\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                # If the counter exceeds the patience, set the early stop flag\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: get_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get a unique log directory\n",
    "def get_log_dir(base_dir='runs'):\n",
    "    \"\"\"\n",
    "    Generates a unique log directory path based on the current date and time.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str, optional): The base directory where logs will be saved. Default is 'runs'.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique directory path for saving logs.\n",
    "    \"\"\"\n",
    "    # Get the current date and time as a formatted string\n",
    "    current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    # Create a log directory path by combining the base directory and the current time\n",
    "    log_dir = os.path.join(base_dir, current_time)\n",
    "    return log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with early stopping\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5, log_dir=None, hparams=None):\n",
    "    \"\"\"\n",
    "    Trains the model with early stopping and logs metrics to TensorBoard.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be trained.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for training.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        num_epochs (int, optional): The maximum number of epochs for training. Default is 25.\n",
    "        patience (int, optional): The number of epochs with no improvement after which training will be stopped. Default is 5.\n",
    "        log_dir (str, optional): The directory to save TensorBoard logs. If None, a new directory will be created.\n",
    "        hparams (dict, optional): Hyperparameters to log.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if log_dir is None:\n",
    "        log_dir = get_log_dir()                         # Create a unique log directory if not provided\n",
    "    writer = SummaryWriter(log_dir=log_dir)             # Initialize TensorBoard writer\n",
    "    early_stopping = EarlyStopping(patience=patience)   # Initialize early stopping\n",
    "    \n",
    "    # Log hyperparameters before training begins\n",
    "    if hparams is not None:\n",
    "        writer.add_hparams(hparams, {})\n",
    "\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()                # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
    "            \n",
    "            optimizer.zero_grad() # Zero the parameter gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)             # Accumulate loss\n",
    "            _, preds = torch.max(outputs, 1)                         # Get predictions\n",
    "            correct_predictions += torch.sum(preds == labels).item() # Count correct predictions\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)             # Calculate average loss for this epoch\n",
    "        epoch_accuracy = correct_predictions / len(train_loader.dataset)  # Calculate accuracy for this epoch\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        val_running_loss = 0.0\n",
    "        val_correct_predictions = 0\n",
    "        with torch.no_grad():                                           # Disable gradient computation for validation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)   # Move inputs and labels to GPU\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item() * inputs.size(0)        # Accumulate validation loss\n",
    "                _, preds = torch.max(outputs, 1)                        # Get predictions\n",
    "                val_correct_predictions += torch.sum(preds == labels).item() # Count correct predictions\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader.dataset)               # Calculate average validation loss\n",
    "        val_accuracy = val_correct_predictions / len(val_loader.dataset)    # Calculate validation accuracy\n",
    "        \n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)                      # Log training loss\n",
    "        writer.add_scalar('Loss/validation', val_loss, epoch)                   # Log validation loss\n",
    "        writer.add_scalar('Accuracy/train', epoch_accuracy, epoch)              # Log training accuracy\n",
    "        writer.add_scalar('Accuracy/validation', val_accuracy, epoch)           # Log validation accuracy\n",
    "        writer.add_scalar('Learning_Rate', scheduler.get_last_lr()[0], epoch)   # Log learning rate\n",
    "        \n",
    "        # Print metrics for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping criteria\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Log final metrics to TensorBoard with hyperparameters\n",
    "    if hparams is not None:\n",
    "        writer.add_hparams(hparams, {'hparam/accuracy': val_accuracy, 'hparam/loss': val_loss})\n",
    "    print('Training complete')\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and prints the test loss and accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained neural network model to be evaluated.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (nn.Module): The loss function.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():                                           # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)   # Move inputs and labels to GPU\n",
    "\n",
    "            outputs = model(inputs)             # Forward pass\n",
    "            loss = criterion(outputs, labels)   # Compute loss\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)    # Accumulate loss\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)                            # Get predictions\n",
    "            correct_predictions += torch.sum(preds == labels).item()    # Count correct predictions\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader.dataset)         # Calculate average test loss\n",
    "    accuracy = correct_predictions / len(test_loader.dataset)   # Calculate test accuracy\n",
    "    \n",
    "    # Print test metrics\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class: PyTorchClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PyTorch classifier for hyperparameter search\n",
    "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Custom PyTorch classifier for hyperparameter search with scikit-learn compatibility.\n",
    "\n",
    "    Args:\n",
    "        hidden_sizes (tuple): Sizes of hidden layers.\n",
    "        dropout_prob (float): Dropout probability.\n",
    "        lr (float): Learning rate.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        patience (int): Patience for early stopping.\n",
    "        log_dir (str): Directory to save TensorBoard logs.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_sizes=(512, 128, 64, 32), dropout_prob=0.5, lr=0.001, num_epochs=3, patience=5, log_dir=None):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.lr = lr\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "        self.log_dir = log_dir\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the PyTorch model on the given dataset.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Training data features.\n",
    "            y (numpy.ndarray): Training data labels.\n",
    "\n",
    "        Returns:\n",
    "            self: Returns an instance of self.\n",
    "        \"\"\"\n",
    "        input_size = X.shape[1]         # Number of input features\n",
    "        output_size = len(np.unique(y)) # Number of unique classes\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = DroughtClassifier(input_size, self.hidden_sizes, output_size, self.dropout_prob).to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.005)\n",
    "\n",
    "        # Create DataLoader for training data\n",
    "        train_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        labels_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        train_dataset = TensorDataset(train_tensor, labels_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        # Define hyperparameters for logging\n",
    "        hparams = {\n",
    "            'hidden_sizes': str(self.hidden_sizes),\n",
    "            'dropout_prob': self.dropout_prob,\n",
    "            'lr': self.lr\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        train_model(self.model, train_loader, val_loader, criterion, optimizer, scheduler, self.num_epochs, self.patience, log_dir=self.log_dir, hparams=hparams)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Data features.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted labels.\n",
    "        \"\"\"\n",
    "        self.model.eval()   # Set model to evaluation mode\n",
    "        test_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        test_loader = DataLoader(test_tensor, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            for inputs in test_loader:\n",
    "                inputs = inputs.to(device)              # Move inputs to GPU\n",
    "                outputs = self.model(inputs)            # Forward pass\n",
    "                _, preds = torch.max(outputs, 1)        # Get predictions\n",
    "                predictions.extend(preds.cpu().numpy()) # Store predictions\n",
    "\n",
    "        # Return predictions as a numpy array\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Base Model, Loss Function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "input_size = X_train.shape[1]       # Number of features\n",
    "hidden_sizes = [512, 128, 64, 32]   \n",
    "\n",
    "output_size = 6                # Number of output classes\n",
    "dropout_prob = 0.5             # Dropout probability\n",
    "\n",
    "# Initialize the model\n",
    "base_model = DroughtClassifier(input_size, hidden_sizes, output_size, dropout_prob).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.005)\n",
    "# writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParameterGrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for RandomizedSearchCV\n",
    "lr_space = [10**(-4 * np.random.uniform(0.5, 1)) for _ in range(5)] #learning rate values between 0.1 (1e-1) and 0.001 (1e-4)\n",
    "param_grid = {\n",
    "    'hidden_sizes': [(512, 128, 64, 32), (256, 64, 32), (512, 256, 128, 64)],\n",
    "    'dropout_prob': [0.3, 0.4, 0.5],\n",
    "    'lr': lr_space\n",
    "}\n",
    "\n",
    "# Initialize ParameterGrid\n",
    "grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_test is a numpy array\n",
    "y_test = y_test.to_numpy() if not isinstance(y_test, np.ndarray) else y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each hyperparameter combination\n",
    "best_model = None\n",
    "best_params = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for params in grid:\n",
    "    # Print the current hyperparameter combination being trained\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "\n",
    "    # Generate a unique log directory for each iteration\n",
    "    log_dir = get_log_dir()\n",
    "\n",
    "    # Create a PyTorchClassifier model with the current hyperparameters\n",
    "    model = PyTorchClassifier(hidden_sizes=params['hidden_sizes'],dropout_prob=params['dropout_prob'],lr=params['lr'],num_epochs=25,patience=3,log_dir=log_dir)\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_predictions = model.predict(X_test)\n",
    "    val_accuracy = accuracy_score(y_test, val_predictions)\n",
    "\n",
    "    # Print the validation accuracy for the current hyperparameters\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Update the best model and parameters if the current model performs better\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and parameters\n",
    "with open('saved_models/parameterGridSearch_bestModel.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open('saved_models/parameterGridSearch_bestParams.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the best model using the test set\n",
    "evaluate_model(best_model.model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and parameters\n",
    "with open('saved_models/best_model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "with open('saved_models/best_params.pkl', 'rb') as f:\n",
    "    best_params = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder variables for the best scheduler parameters\n",
    "best_step_lr_params = None\n",
    "best_step_lr_val_accuracy = 0\n",
    "\n",
    "best_cosine_annealing_lr_params = None\n",
    "best_cosine_annealing_lr_val_accuracy = 0\n",
    "\n",
    "best_cosine_annealing_lr_warm_params = None\n",
    "best_cosine_annealing_lr_warm_val_accuracy = 0\n",
    "\n",
    "# Define the grid of StepLR parameters to search\n",
    "step_lr_grid = [\n",
    "    {'scheduler_type': 'StepLR', 'step_size': 10, 'gamma': 0.25},\n",
    "    {'scheduler_type': 'StepLR', 'step_size': 10, 'gamma': 0.5},\n",
    "    {'scheduler_type': 'StepLR', 'step_size': 20, 'gamma': 0.25},\n",
    "    {'scheduler_type': 'StepLR', 'step_size': 20, 'gamma': 0.5},\n",
    "]\n",
    "\n",
    "# Define the grid of CosineAnnealingLR parameters to search\n",
    "cosine_annealing_lr_grid = [\n",
    "    {'scheduler_type': 'CosineAnnealingLR', 'T_max': 10, 'eta_min': 1e-5},\n",
    "    {'scheduler_type': 'CosineAnnealingLR', 'T_max': 20, 'eta_min': 1e-5},\n",
    "    {'scheduler_type': 'CosineAnnealingLR', 'T_max': 10, 'eta_min': 1e-6},\n",
    "    {'scheduler_type': 'CosineAnnealingLR', 'T_max': 20, 'eta_min': 1e-6},\n",
    "]\n",
    "cosine_annealing_warm_lr_grid = [\n",
    "    {'scheduler_type': 'CosineAnnealingLRWarm', 'T_0': 10, 'eta_min': 1e-4},\n",
    "    {'scheduler_type': 'CosineAnnealingLRWarm', 'T_0': 20, 'eta_min': 1e-4},\n",
    "    {'scheduler_type': 'CosineAnnealingLRWarm', 'T_0': 10, 'eta_min': 1e-5},\n",
    "    {'scheduler_type': 'CosineAnnealingLRWarm', 'T_0': 20, 'eta_min': 1e-5},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the scheduler based on parameters\n",
    "def create_scheduler(optimizer, params):\n",
    "    if params['scheduler_type'] == 'StepLR':\n",
    "        return StepLR(optimizer, step_size=params['step_size'], gamma=params['gamma'])\n",
    "    elif params['scheduler_type'] == 'CosineAnnealingLR':\n",
    "        return CosineAnnealingLR(optimizer, T_max=params['T_max'], eta_min=params['eta_min'])\n",
    "    elif params['scheduler_type'] == 'CosineAnnealingLRWarm':\n",
    "        return CosineAnnealingWarmRestarts(optimizer, \n",
    "                                        T_0 = params['T_0'],            # Number of iterations for the first restart\n",
    "                                        T_mult = 1,                     # A factor increases TiTi​ after a restart\n",
    "                                        eta_min = params['eta_min'])    # Minimum learning rate\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler type: {params['scheduler_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for StepLR parameters\n",
    "for scheduler_params in step_lr_grid:\n",
    "    print(f\"Tuning with StepLR parameters: {scheduler_params}\")\n",
    "    \n",
    "    # Initialize a new model for each scheduler parameter combination\n",
    "    model = DroughtClassifier(input_size, best_params['hidden_sizes'], output_size, best_params['dropout_prob']).to(device)\n",
    "    \n",
    "    # Create a new optimizer and scheduler with the current scheduler parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "    scheduler = create_scheduler(optimizer, scheduler_params)\n",
    "\n",
    "    # Train the model with the new scheduler parameters\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=100, patience=5, log_dir=get_log_dir(), hparams=scheduler_params)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_preds += (predicted == labels).sum().item()\n",
    "            val_total_preds += labels.size(0)\n",
    "    \n",
    "    val_accuracy = val_correct_preds / val_total_preds\n",
    "    print(f\"Validation Accuracy with StepLR parameters: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Update the best StepLR parameters if the current model performs better\n",
    "    if val_accuracy > best_step_lr_val_accuracy:\n",
    "        best_step_lr_val_accuracy = val_accuracy\n",
    "        best_step_lr_params = scheduler_params\n",
    "\n",
    "print(f\"Best StepLR parameters: {best_step_lr_params}\")\n",
    "print(f\"Best StepLR validation accuracy: {best_step_lr_val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and parameters\n",
    "with open('saved_models/stepLRSearch_bestParams.pkl', 'wb') as f:\n",
    "    pickle.dump(best_step_lr_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved best model\n",
    "with open('saved_models/stepLRSearch_bestParams.pkl', 'rb') as f:\n",
    "    best_step_lr_params = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded Best Hyperparameters: {best_step_lr_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for CosineAnnealingLR parameters\n",
    "for scheduler_params in cosine_annealing_lr_grid:\n",
    "    print(f\"Tuning with CosineAnnealingLR parameters: {scheduler_params}\")\n",
    "    \n",
    "    # Initialize a new model for each scheduler parameter combination\n",
    "    model = DroughtClassifier(input_size, best_params['hidden_sizes'], output_size, best_params['dropout_prob']).to(device)\n",
    "    \n",
    "    # Create a new optimizer and scheduler with the current scheduler parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "    scheduler = create_scheduler(optimizer, scheduler_params)\n",
    "\n",
    "    # Train the model with the new scheduler parameters\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5, log_dir=get_log_dir(), hparams=scheduler_params)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_preds += (predicted == labels).sum().item()\n",
    "            val_total_preds += labels.size(0)\n",
    "    \n",
    "    val_accuracy = val_correct_preds / val_total_preds\n",
    "    print(f\"Validation Accuracy with CosineAnnealingLR parameters: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Update the best CosineAnnealingLR parameters if the current model performs better\n",
    "    if val_accuracy > best_cosine_annealing_lr_val_accuracy:\n",
    "        best_cosine_annealing_lr_val_accuracy = val_accuracy\n",
    "        best_cosine_annealing_lr_params = scheduler_params\n",
    "\n",
    "print(f\"Best CosineAnnealingLR parameters: {best_cosine_annealing_lr_params}\")\n",
    "print(f\"Best CosineAnnealingLR validation accuracy: {best_cosine_annealing_lr_val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "with open('saved_models/CosineAnnealingLR_bestParams.pkl', 'wb') as f:\n",
    "    pickle.dump(best_cosine_annealing_lr_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "with open('saved_models/CosineAnnealingLR_bestParams.pkl', 'rb') as f:\n",
    "    best_cosine_annealing_lr_params = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded Best Hyperparameters: {best_cosine_annealing_lr_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CosineAnnealingLR_Warm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for CosineAnnealingLR_warm parameters\n",
    "for scheduler_params in cosine_annealing_warm_lr_grid:\n",
    "    print(f\"Tuning with CosineAnnealingLR_warm parameters: {scheduler_params}\")\n",
    "    \n",
    "    # Initialize a new model for each scheduler parameter combination\n",
    "    model = DroughtClassifier(input_size, best_params['hidden_sizes'], output_size, best_params['dropout_prob']).to(device)\n",
    "    \n",
    "    # Create a new optimizer and scheduler with the current scheduler parameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "    scheduler = create_scheduler(optimizer, scheduler_params)\n",
    "\n",
    "    # Train the model with the new scheduler parameters\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=100, patience=5, log_dir=get_log_dir(), hparams=scheduler_params)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    val_correct_preds = 0\n",
    "    val_total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_preds += (predicted == labels).sum().item()\n",
    "            val_total_preds += labels.size(0)\n",
    "    \n",
    "    val_accuracy = val_correct_preds / val_total_preds\n",
    "    print(f\"Validation Accuracy with CosineAnnealingLR parameters: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Update the best CosineAnnealingLR parameters if the current model performs better\n",
    "    if val_accuracy > best_cosine_annealing_lr_val_accuracy:\n",
    "        best_cosine_annealing_lr_warm_val_accuracy = val_accuracy\n",
    "        best_cosine_annealing_lr_warm_params = scheduler_params\n",
    "\n",
    "print(f\"Best CosineAnnealingLR parameters: {best_cosine_annealing_lr_warm_params}\")\n",
    "print(f\"Best CosineAnnealingLR validation accuracy: {best_cosine_annealing_lr_warm_val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "with open('saved_models/CosineAnnealingLR_warm_bestParams.pkl', 'wb') as f:\n",
    "    pickle.dump(best_cosine_annealing_lr_warm_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved best model\n",
    "with open('saved_models/CosineAnnealingLR_warm_bestParams.pkl', 'rb') as f:\n",
    "    best_cosine_annealing_lr_warm_params = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded Best Hyperparameters: {best_cosine_annealing_lr_warm_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation datasets\n",
    "full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=512, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the best model with the best hyperparameters\n",
    "retrained_model_stepLR = DroughtClassifier(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_params['hidden_sizes'],\n",
    "    output_size=output_size,\n",
    "    dropout_prob=best_params['dropout_prob']\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Define the criterion, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(retrained_model_stepLR.parameters(), lr=best_params['lr'])\n",
    "scheduler = create_scheduler(optimizer, best_step_lr_params)\n",
    "\n",
    "print(retrained_model_stepLR)\n",
    "print(best_params)\n",
    "print(best_step_lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model on the full training dataset\n",
    "print(\"Retraining the best model on the full training dataset...\")\n",
    "\n",
    "train_model(retrained_model_stepLR, full_train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=1000, patience=10, log_dir=get_log_dir(), hparams=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the retrained model using the test set\n",
    "evaluate_model(retrained_model_stepLR, test_loader, criterion)\n",
    "\n",
    "# Save the retrained model\n",
    "with open('saved_models/retrained_model_stepLR.pkl', 'wb') as f:\n",
    "    pickle.dump(retrained_model_stepLR, f)\n",
    "\n",
    "print(\"Retraining complete. Model saved as 'retrained_model_stepLR.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retrained model\n",
    "with open('saved_models/retrained_model_stepLR.pkl', 'rb') as f:\n",
    "    retrained_model_stepLR = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CosineAnnealingLRWarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_models/CosineAnnealingLR_warm_bestParams.pkl', 'rb') as f:\n",
    "    best_cosine_annealing_lr_params = pickle.load(f)\n",
    "\n",
    "with open('saved_models/parameterGridSearch_bestParams.pkl', 'rb') as f:\n",
    "    best_params = pickle.load(f)\n",
    "\n",
    "\n",
    "print(best_cosine_annealing_lr_params)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation datasets\n",
    "full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=512, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the best model with the best hyperparameters\n",
    "retrained_model_CosineAnnealingLRWarm = DroughtClassifier(\n",
    "    input_size=input_size,\n",
    "    hidden_sizes=best_params['hidden_sizes'],\n",
    "    output_size=output_size,\n",
    "    dropout_prob=best_params['dropout_prob']\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Define the criterion, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(retrained_model_CosineAnnealingLRWarm.parameters(), lr=best_params['lr'])\n",
    "scheduler = create_scheduler(optimizer, best_cosine_annealing_lr_params)\n",
    "\n",
    "print(retrained_model_CosineAnnealingLRWarm)\n",
    "print(best_params)\n",
    "print(best_cosine_annealing_lr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model on the full training dataset\n",
    "print(\"Retraining the best model on the full training dataset...\")\n",
    "\n",
    "train_model(retrained_model_CosineAnnealingLRWarm, full_train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=1000, patience=10, log_dir=get_log_dir(), hparams=best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the retrained model using the test set\n",
    "evaluate_model(retrained_model_CosineAnnealingLRWarm, test_loader, criterion)\n",
    "\n",
    "# Save the retrained model\n",
    "with open('saved_models/retrained_model_CosineAnnealingLRWarm2.pkl', 'wb') as f:\n",
    "    pickle.dump(retrained_model_CosineAnnealingLRWarm, f)\n",
    "\n",
    "print(\"Retraining complete. Model saved as 'retrained_model_CosineAnnealingLRWarm2.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retrained model\n",
    "with open('saved_models/retrained_model_CosineAnnealingLRWarm2.pkl', 'rb') as f:\n",
    "    retrained_model_CosineAnnealingLRWarm = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
