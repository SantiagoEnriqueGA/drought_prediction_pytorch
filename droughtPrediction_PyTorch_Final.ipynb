{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook shows the final model after tuning and training in the notebook `droughtPrediction_PyTorch_HP.ipynb`.  \n",
    "Below it is evaluated on the test dataset and a data pipeline created for predicting on the original data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn libraries for metrics\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device, use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DroughtClassifier` class is a neural network designed in `droughtPrediction_PyTorch_HP.ipynb` for drought prediction. It consists of multiple linear layers with ReLU activation and dropout for regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network classifier for drought prediction.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input features.\n",
    "        hidden_sizes (list of int): A list containing the sizes of the hidden layers.\n",
    "        output_size (int): The number of output classes.\n",
    "        dropout_prob (float, optional): The probability of an element to be zeroed in dropout. Default is 0.5.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): A list of linear layers.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.5):\n",
    "        super(DroughtClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        # Apply each layer followed by ReLU activation and dropout, except the last layer\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.dropout(F.relu(layer(x)))\n",
    "        # Apply the last layer without activation or dropout\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluate_model` function evaluates the model on the test dataset and prints metrics including loss, accuracy, Macro F1 Mean, and MAE Mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and prints the test loss, accuracy, Macro F1 Mean, and MAE Mean.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained neural network model to be evaluated.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (nn.Module): The loss function.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():                                           # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)   # Move inputs and labels to GPU\n",
    "\n",
    "            outputs = model(inputs)             # Forward pass\n",
    "            loss = criterion(outputs, labels)   # Compute loss\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)    # Accumulate loss\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)                            # Get predictions\n",
    "            correct_predictions += torch.sum(preds == labels).item()    # Count correct predictions\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())                     # Collect all labels\n",
    "            all_preds.extend(preds.cpu().numpy())                       # Collect all predictions\n",
    "\n",
    "    test_loss = running_loss / len(test_loader.dataset)         # Calculate average test loss\n",
    "    accuracy = correct_predictions / len(test_loader.dataset)   # Calculate test accuracy\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro') # Calculate Macro F1 Mean\n",
    "    mae = mean_absolute_error(all_labels, all_preds)            # Calculate MAE Mean\n",
    "    \n",
    "    # Print test metrics\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, Macro F1 Mean: {macro_f1:.4f}, MAE Mean: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `format_input_for_model` formats data for prediction in the model by performing the same steps taken in `droughtPrediction_DataEng.ipynb`.  \n",
    "This is done by saving and loading the `StandardScaler` and `PCA` objects used on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_for_model(input_series, scaler_file='data/scaler.pkl', pca_model_file='data/pca_model.pkl'):\n",
    "    \"\"\"\n",
    "    Format input data series for model prediction.\n",
    "\n",
    "    Args:\n",
    "    - input_series (pd.Series): Input series containing data to be formatted.\n",
    "    - scaler_file (str): File path to the saved StandardScaler object.\n",
    "    - pca_model_file (str): File path to the saved PCA model object.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Transformed and scaled input data ready for model prediction.\n",
    "    \"\"\"\n",
    "    # Step 1: Merge input_series with soil_df based on 'fips'\n",
    "    soil_df = pd.read_csv('data/soil_data.csv')\n",
    "    input_data = pd.DataFrame(input_series).T.merge(soil_df, on='fips', how='left')\n",
    "    \n",
    "    # Step 2: Drop unnecessary columns 'date' and 'fips' if they exist\n",
    "    input_data.drop(columns=['date', 'fips'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Step 3: Load the saved StandardScaler object\n",
    "    with open(scaler_file, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    \n",
    "    # Step 4: Scale the input data\n",
    "    scaled_data = scaler.transform(input_data)\n",
    "    \n",
    "    # Step 5: Load the saved PCA object and apply transformation\n",
    "    with open(pca_model_file, 'rb') as file:\n",
    "        pca_model = pickle.load(file)\n",
    "    \n",
    "    pca_transformed = pca_model.transform(scaled_data)\n",
    "    \n",
    "    return pca_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `predict_formatted_input` performs the actual prediction of the data using a PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format input and make predictions\n",
    "def predict_formatted_input(model, input_series):\n",
    "    \"\"\"\n",
    "    Format input data series, make predictions using a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): PyTorch model to use for predictions.\n",
    "    - input_series (pd.Series): Input series containing data to be formatted and predicted.\n",
    "\n",
    "    Returns:\n",
    "    - int: Predicted class index.\n",
    "    \"\"\"\n",
    "    # Step 1: Format input for model\n",
    "    formatted_input = format_input_for_model(input_series)\n",
    "    \n",
    "    # Step 2: Convert formatted_input to torch tensor\n",
    "    input_tensor = torch.tensor(formatted_input, dtype=torch.float32)\n",
    "    \n",
    "    # Step 3: Ensure model is in evaluation mode and on CPU\n",
    "    model.eval()\n",
    "    model.cpu()  # Move model to CPU explicitly\n",
    "    \n",
    "    # Step 4: Move input_tensor to CPU if it's not already\n",
    "    input_tensor = input_tensor.cpu()\n",
    "    \n",
    "    # Step 5: Perform prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)  # Assuming classification task, get predicted class index\n",
    "    \n",
    "    return predicted.item()  # Return the predicted class as an integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retrained model\n",
    "with open('saved_models//retrained_model_stepLR2.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the original data as well as the test dataset.  \n",
    "We also convert the test dataset to PyTorch tensors and create DataLoaders to evaluate the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "\n",
    "# Load training and testing data from a pickle file\n",
    "with open('data/Xy_trainTest.pkl', 'rb') as f:\n",
    "    # Unpickle the data into training and testing datasets\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Statistincs on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Metrics on test dataset:\n",
      "Test Loss: 0.6352, Accuracy: 0.7337, Macro F1 Mean: 0.6895, MAE Mean: 0.3255\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f'Model Metrics on test dataset:')\n",
    "evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilize Model pipline to predict original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(row):\n",
    "    input_series = drought_df.drop(columns=['score']).iloc[row]\n",
    "    true_score = drought_df['score'].iloc[row]\n",
    "\n",
    "    # Predict with the model\n",
    "    predicted_class = predict_formatted_input(model, input_series)\n",
    "\n",
    "    # Display input_series and prediction\n",
    "    print(f\"Input Series:\\n{input_series}\")\n",
    "    print(f\"\\nPredicted Class: {predicted_class}\")\n",
    "    print(f\"True Class     : {true_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Series:\n",
      "fips                 1001\n",
      "date           2000-01-11\n",
      "PRECTOT              1.33\n",
      "PS                  100.4\n",
      "QV2M                 6.63\n",
      "T2M                 11.48\n",
      "T2MDEW               7.84\n",
      "T2MWET               7.84\n",
      "T2M_MAX             18.88\n",
      "T2M_MIN              5.72\n",
      "T2M_RANGE           13.16\n",
      "TS                  10.43\n",
      "WS10M                1.76\n",
      "WS10M_MAX            2.48\n",
      "WS10M_MIN            1.05\n",
      "WS10M_RANGE          1.43\n",
      "WS50M                3.55\n",
      "WS50M_MAX            6.38\n",
      "WS50M_MIN            1.71\n",
      "WS50M_RANGE          4.67\n",
      "year                 2000\n",
      "month                   1\n",
      "day                    11\n",
      "Name: 1, dtype: object\n",
      "\n",
      "Predicted Class: 2\n",
      "True Class     : 2\n"
     ]
    }
   ],
   "source": [
    "get_pred(row=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchviz Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model using torchviz\n",
    "def visualize_model(model, input_size):\n",
    "    model.eval()\n",
    "    x = torch.randn(1, input_size)  # Create a dummy input tensor\n",
    "    y = model(x)  # Perform a forward pass through the model\n",
    "    make_dot(y, params=dict(model.named_parameters())).render(\"drought_model_simp\", format=\"png\")  \n",
    "    make_dot(y, params=dict(model.named_parameters()), show_attrs=True, show_saved=True).render(\"drought_model_comp\", format=\"png\")  \n",
    "\n",
    "# Assuming input_size is known (e.g., from X_test.shape[1])\n",
    "input_size = X_test.shape[1]\n",
    "visualize_model(model, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "drought_model_simp `DroughtClassifier` model\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - The input tensor has a shape of \\((1, 52)\\), indicating a batch size of 1 and 52 input features.\n",
    "\n",
    "2. **First Linear Layer (layers.0):**\n",
    "   - **weights:** `layers.0.weight (1024, 52)`\n",
    "   - **bias:** `layers.0.bias (1024)`\n",
    "   - This layer maps the 52 input features to 1024 features using a linear transformation.\n",
    "\n",
    "3. **First Activation and Dropout:**\n",
    "   - The output from the first linear layer passes through a ReLU activation function, followed by a dropout layer to introduce regularization.\n",
    "   - Represented by `ReLUBackward0` and `TBackward0`.\n",
    "\n",
    "4. **Second Linear Layer (layers.1):**\n",
    "   - **weights:** `layers.1.weight (512, 1024)`\n",
    "   - **bias:** `layers.1.bias (512)`\n",
    "   - This layer takes the 1024 features from the previous layer and maps them to 512 features.\n",
    "\n",
    "5. **Second Activation and Dropout:**\n",
    "   - Similar to the first layer, the output from the second linear layer passes through ReLU activation and dropout.\n",
    "   - Represented by `ReLUBackward0` and `TBackward0`.\n",
    "\n",
    "6. **Third Linear Layer (layers.2):**\n",
    "   - **weights:** `layers.2.weight (256, 512)`\n",
    "   - **bias:** `layers.2.bias (256)`\n",
    "   - This layer reduces the 512 features to 256 features.\n",
    "\n",
    "7. **Third Activation and Dropout:**\n",
    "   - Again, the output goes through ReLU activation and dropout.\n",
    "   - Represented by `ReLUBackward0` and `TBackward0`.\n",
    "\n",
    "8. **Fourth Linear Layer (layers.3):**\n",
    "   - **weights:** `layers.3.weight (128, 256)`\n",
    "   - **bias:** `layers.3.bias (128)`\n",
    "   - This layer further reduces the features from 256 to 128.\n",
    "\n",
    "9. **Fourth Activation and Dropout:**\n",
    "   - The output undergoes ReLU activation and dropout.\n",
    "   - Represented by `ReLUBackward0` and `TBackward0`.\n",
    "\n",
    "10. **Fifth (Output) Linear Layer (layers.4):**\n",
    "    - **weights:** `layers.4.weight (6, 128)`\n",
    "    - **bias:** `layers.4.bias (6)`\n",
    "    - This final layer maps the 128 features to 6 output classes (assuming a 6-class classification problem).\n",
    "\n",
    "11. **Output:**\n",
    "    - The final output tensor has a shape of \\((1, 6)\\), indicating the model's prediction probabilities for each of the 6 classes.\n",
    "\n",
    "**AccumulateGrad Nodes:**\n",
    "- These nodes represent where the gradients are accumulated during the backward pass for updating the model parameters.\n",
    "\n",
    "**AddmmBackward0 Nodes:**\n",
    "- These nodes represent the matrix multiplication operations performed during the forward pass through each linear layer.\n",
    "- Detailed information about the matrix multiplication is provided, including the sizes of the matrices involved (`mat1`, `mat2`), strides, and saved tensors.\n",
    "\n",
    "**ReluBackward0 and TBackward0 Nodes:**\n",
    "- These nodes represent the ReLU activation and dropout operations applied to the output of each linear layer.\n",
    "\n",
    "**mat1, mat2, and result Tensors:**\n",
    "- `mat1` and `mat2` represent the input and weight matrices involved in the matrix multiplication.\n",
    "- `result` represents the output tensor after each matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
