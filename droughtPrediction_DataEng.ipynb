{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drought_df_train = pd.read_csv('data/train_timeseries.csv')\n",
    "# drought_df_test = pd.read_csv('data/test_timeseries.csv')\n",
    "# drought_df_test = pd.read_csv('data/validation_timeseries.csv')\n",
    "\n",
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "drought_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_df = pd.read_csv('data/soil_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter outliers based on the 3-sigma rule\n",
    "def remove_outliers(df, columns):\n",
    "    \"\"\"\n",
    "    Remove outliers from the specified columns in a DataFrame using the 3-sigma rule.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    columns (list of str): The list of columns to apply the outlier removal.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        # Calculate the mean and standard deviation for the current column\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "\n",
    "        # Calculate the upper and lower limits using the 3-sigma rule\n",
    "        upper_limit = mean + 3 * std\n",
    "        lower_limit = mean - 3 * std\n",
    "\n",
    "        # Filter the DataFrame to exclude rows where the column value is outside the 3-sigma limits\n",
    "        df = df[(df[col] <= upper_limit) & (df[col] >= lower_limit)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding Categorical cols\n",
    "measures = ['PRECTOT','PS','QV2M','T2M','T2MDEW','T2MWET','T2M_MAX','T2M_MIN','T2M_RANGE','TS','WS10M','WS10M_MAX','WS10M_MIN','WS10M_RANGE','WS50M','WS50M_MAX','WS50M_MIN','WS50M_RANGE']\n",
    "\n",
    "# Remove outliers\n",
    "cleaned_drought_df = remove_outliers(drought_df, measures)\n",
    "\n",
    "# Print the number of rows before and after removing outliers\n",
    "print(f'Total rows before removing outliers: {len(drought_df)}')\n",
    "print(f'Total rows after removing outliers: {len(cleaned_drought_df)}')\n",
    "print(f'Number of outliers: {len(drought_df)-len(cleaned_drought_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine timeseries and soil data\n",
    "combined_df = cleaned_drought_df.merge(soil_df, how='left', on='fips')\n",
    "\n",
    "# Drop fips code and date\n",
    "combined_df.drop(columns=['fips','date'], inplace=True)\n",
    "\n",
    "# List columns\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to X,y train,test\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df.drop(columns=['score']), combined_df['score'], test_size=0.2, random_state=42)   \n",
    "\n",
    "print(\"Train features shape\", X_train.shape)\n",
    "print(\"Train target shape\", y_train.shape)\n",
    "print(\"Test features shape\", X_test.shape)\n",
    "print(\"Test target shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing features by to mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train) # Fit the scaler on the training data and transform the training data\n",
    "X_test = scaler.transform(X_test)       # Transform the test data using the fitted scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler object to a file\n",
    "with open('data/scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling using SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "sm = SMOTE(random_state = 1) # Instance of the SMOTE class\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_ures_SMOTE, y_train_ures_SMOTE = sm.fit_resample(X_train, y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before OverSampling, the shape of train_X: {}'.format(X_train.shape))\n",
    "print('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_ures_SMOTE.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_ures_SMOTE.shape))\n",
    "\n",
    "print(\"Counts of label '0' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 0),sum(y_train_ures_SMOTE == 0)))\n",
    "print(\"Counts of label '1' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 1),sum(y_train_ures_SMOTE == 1)))\n",
    "print(\"Counts of label '2' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 2),sum(y_train_ures_SMOTE == 2)))\n",
    "print(\"Counts of label '3' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 3),sum(y_train_ures_SMOTE == 3)))\n",
    "print(\"Counts of label '4' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 4),sum(y_train_ures_SMOTE == 4)))\n",
    "print(\"Counts of label '5' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 5),sum(y_train_ures_SMOTE == 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()     # Instance of the PCA class\n",
    "\n",
    "X_train_ures_SMOTE_PCAreduced = pca.fit_transform(X_train_ures_SMOTE)   # Fit PCA on the upsampled training data and transform it\n",
    "X_test_SMOTE_PCA_transformed = pca.transform(X_test)                    # Transform the test data using the fitted PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the PCA object to a file\n",
    "with open('data/pca_model.pkl', 'wb') as file:\n",
    "    pickle.dump(pca, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative explained variance ratio\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine number of components that explain 95% of variance\n",
    "threshold = 0.95\n",
    "num_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.grid()\n",
    "\n",
    "# Add vertical line for num_components\n",
    "plt.axvline(x=num_components - 1, color='r', linestyle='--', label=f'{num_components} components ({threshold * 100:.1f}% variance)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train Shape: {X_train_ures_SMOTE_PCAreduced.shape}\")\n",
    "print(f\"y_train Shape: {y_train_ures_SMOTE.shape}\")\n",
    "\n",
    "print(f\"X_test  Shape: {X_test_SMOTE_PCA_transformed.shape}\")\n",
    "print(f\"y_test  Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('data\\Xy_trainTest.pkl', 'wb') as f:  \n",
    "    pickle.dump([X_train_ures_SMOTE_PCAreduced, X_test_SMOTE_PCA_transformed, \n",
    "                 y_train_ures_SMOTE, y_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting back the objects:\n",
    "with open('data\\Xy_trainTest.pkl', 'rb') as f:  \n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR MODEL INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_for_model(input_series, scaler_file='data/scaler.pkl', pca_model_file='data/pca_model.pkl'):\n",
    "    # Step 1: Merge input_series with soil_df based on 'fips'\n",
    "    soil_df = pd.read_csv('data/soil_data.csv')\n",
    "    input_data = pd.DataFrame(input_series).T.merge(soil_df, on='fips', how='left')\n",
    "    \n",
    "    # Step 2: Drop unnecessary columns 'date' and 'fips' if they exist\n",
    "    input_data.drop(columns=['date', 'fips'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Step 3: Load the saved StandardScaler object\n",
    "    with open(scaler_file, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    \n",
    "    # Step 4: Scale the input data\n",
    "    scaled_data = scaler.transform(input_data)\n",
    "    \n",
    "    # Step 5: Load the saved PCA object and apply transformation\n",
    "    with open(pca_model_file, 'rb') as file:\n",
    "        pca_model = pickle.load(file)\n",
    "    \n",
    "    pca_transformed = pca_model.transform(scaled_data)\n",
    "    \n",
    "    return pca_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Series:\n",
      "fips                 1001\n",
      "date           2000-01-04\n",
      "PRECTOT             15.95\n",
      "PS                 100.29\n",
      "QV2M                 6.42\n",
      "T2M                  11.4\n",
      "T2MDEW               6.09\n",
      "T2MWET                6.1\n",
      "T2M_MAX             18.09\n",
      "T2M_MIN              2.16\n",
      "T2M_RANGE           15.92\n",
      "TS                  11.31\n",
      "WS10M                3.84\n",
      "WS10M_MAX            5.67\n",
      "WS10M_MIN            2.08\n",
      "WS10M_RANGE          3.59\n",
      "WS50M                6.73\n",
      "WS50M_MAX            9.31\n",
      "WS50M_MIN            3.74\n",
      "WS50M_RANGE          5.58\n",
      "year                 2000\n",
      "month                   1\n",
      "day                     4\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Formatted Input After Preprocessing:\n",
      "[[ 7.04522582e-01 -2.92456168e-01 -6.33846229e-01 -1.02320858e+00\n",
      "   1.09296898e+00  3.05310122e-01 -9.57448849e-02  1.70161112e+00\n",
      "  -1.86575326e+00  1.42976851e+00  3.25090609e+00  4.10658237e-01\n",
      "  -9.35472833e-01 -1.49237342e+00 -1.56141952e+00 -1.11877206e-01\n",
      "  -1.56664738e+00  1.40051927e+00  6.68006002e-01  6.59365594e-01\n",
      "   2.30423427e+00 -1.84236845e+00 -8.52416669e-01 -6.95683016e-01\n",
      "   4.94550544e-01 -5.44391968e-01  3.56821415e-02  2.58053717e-01\n",
      "   6.96958375e-02  2.35130912e-01  2.68892548e-02  1.67134456e-01\n",
      "  -7.08507109e-02 -5.32665282e-01 -3.65443041e-01  7.89557398e-02\n",
      "  -1.15675114e-01  1.28282564e-01 -2.97126797e-01 -2.59504803e-01\n",
      "   2.36885462e-01  5.75274238e-02 -3.41317426e-02  1.01241679e-01\n",
      "   5.32934304e-02 -3.25830790e-04  5.04939081e-05 -2.81360691e-03\n",
      "   6.23652657e-04  2.74900287e-10  3.14107275e-08 -2.91753557e-10]]\n",
      "\n",
      "Shapes:\n",
      "Input Series Shape: (23,)\n",
      "Formatted Input Shape: (1, 52)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "\n",
    "input_series = drought_df.drop(columns=['score']).iloc[0]  # Example input data\n",
    "formatted_input = format_input_for_model(input_series)\n",
    "\n",
    "# Display input_series and formatted_input\n",
    "print(\"Input Series:\")\n",
    "print(input_series)\n",
    "print(\"\\nFormatted Input After Preprocessing:\")\n",
    "print(formatted_input)\n",
    "print(\"\\nShapes:\")\n",
    "print(f\"Input Series Shape: {input_series.shape}\")\n",
    "print(f\"Formatted Input Shape: {formatted_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
