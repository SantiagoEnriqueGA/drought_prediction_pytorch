{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>date</th>\n",
       "      <th>PRECTOT</th>\n",
       "      <th>PS</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>T2M</th>\n",
       "      <th>T2MDEW</th>\n",
       "      <th>T2MWET</th>\n",
       "      <th>T2M_MAX</th>\n",
       "      <th>T2M_MIN</th>\n",
       "      <th>...</th>\n",
       "      <th>WS10M_MIN</th>\n",
       "      <th>WS10M_RANGE</th>\n",
       "      <th>WS50M</th>\n",
       "      <th>WS50M_MAX</th>\n",
       "      <th>WS50M_MIN</th>\n",
       "      <th>WS50M_RANGE</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>15.95</td>\n",
       "      <td>100.29</td>\n",
       "      <td>6.42</td>\n",
       "      <td>11.40</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.10</td>\n",
       "      <td>18.09</td>\n",
       "      <td>2.16</td>\n",
       "      <td>...</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.59</td>\n",
       "      <td>6.73</td>\n",
       "      <td>9.31</td>\n",
       "      <td>3.74</td>\n",
       "      <td>5.58</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000-01-11</td>\n",
       "      <td>1.33</td>\n",
       "      <td>100.40</td>\n",
       "      <td>6.63</td>\n",
       "      <td>11.48</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.84</td>\n",
       "      <td>18.88</td>\n",
       "      <td>5.72</td>\n",
       "      <td>...</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.43</td>\n",
       "      <td>3.55</td>\n",
       "      <td>6.38</td>\n",
       "      <td>1.71</td>\n",
       "      <td>4.67</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000-01-18</td>\n",
       "      <td>1.11</td>\n",
       "      <td>100.39</td>\n",
       "      <td>9.53</td>\n",
       "      <td>14.28</td>\n",
       "      <td>13.26</td>\n",
       "      <td>13.26</td>\n",
       "      <td>18.04</td>\n",
       "      <td>8.98</td>\n",
       "      <td>...</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.92</td>\n",
       "      <td>5.19</td>\n",
       "      <td>6.40</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000-01-25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.11</td>\n",
       "      <td>2.05</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>-7.72</td>\n",
       "      <td>5.65</td>\n",
       "      <td>-5.46</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.32</td>\n",
       "      <td>5.75</td>\n",
       "      <td>8.03</td>\n",
       "      <td>3.96</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>101.00</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2.06</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>11.02</td>\n",
       "      <td>-4.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.86</td>\n",
       "      <td>4.18</td>\n",
       "      <td>6.38</td>\n",
       "      <td>1.27</td>\n",
       "      <td>5.11</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fips        date  PRECTOT      PS  QV2M    T2M  T2MDEW  T2MWET  T2M_MAX  \\\n",
       "0  1001  2000-01-04    15.95  100.29  6.42  11.40    6.09    6.10    18.09   \n",
       "1  1001  2000-01-11     1.33  100.40  6.63  11.48    7.84    7.84    18.88   \n",
       "2  1001  2000-01-18     1.11  100.39  9.53  14.28   13.26   13.26    18.04   \n",
       "3  1001  2000-01-25     0.00  100.11  2.05  -0.78   -7.93   -7.72     5.65   \n",
       "4  1001  2000-02-01     0.00  101.00  3.36   2.06   -1.73   -1.70    11.02   \n",
       "\n",
       "   T2M_MIN  ...  WS10M_MIN  WS10M_RANGE  WS50M  WS50M_MAX  WS50M_MIN  \\\n",
       "0     2.16  ...       2.08         3.59   6.73       9.31       3.74   \n",
       "1     5.72  ...       1.05         1.43   3.55       6.38       1.71   \n",
       "2     8.98  ...       1.67         1.92   5.19       6.40       3.84   \n",
       "3    -5.46  ...       2.28         2.32   5.75       8.03       3.96   \n",
       "4    -4.21  ...       0.88         1.86   4.18       6.38       1.27   \n",
       "\n",
       "   WS50M_RANGE  score  year  month  day  \n",
       "0         5.58      1  2000      1    4  \n",
       "1         4.67      2  2000      1   11  \n",
       "2         2.55      2  2000      1   18  \n",
       "3         4.07      2  2000      1   25  \n",
       "4         5.11      1  2000      2    1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drought_df_train = pd.read_csv('data/train_timeseries.csv')\n",
    "# drought_df_test = pd.read_csv('data/test_timeseries.csv')\n",
    "# drought_df_test = pd.read_csv('data/validation_timeseries.csv')\n",
    "\n",
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "drought_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fips', 'date', 'PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
       "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
       "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
       "       'WS50M_RANGE', 'score', 'year', 'month', 'day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drought_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_df = pd.read_csv('data/soil_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter outliers based on the 3-sigma rule\n",
    "def remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        upper_limit = mean + 3 * std\n",
    "        lower_limit = mean - 3 * std\n",
    "        df = df[(df[col] <= upper_limit) & (df[col] >= lower_limit)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows before removing outliers: 3403260\n",
      "Total rows after removing outliers: 3054658\n",
      "Number of outliers: 348602\n"
     ]
    }
   ],
   "source": [
    "# Avoiding Categorical cols\n",
    "measures = ['PRECTOT','PS','QV2M','T2M','T2MDEW','T2MWET','T2M_MAX','T2M_MIN','T2M_RANGE','TS','WS10M','WS10M_MAX','WS10M_MIN','WS10M_RANGE','WS50M','WS50M_MAX','WS50M_MIN','WS50M_RANGE']\n",
    "\n",
    "# Remove outliers\n",
    "cleaned_drought_df = remove_outliers(drought_df, measures)\n",
    "\n",
    "# Print the number of rows before and after removing outliers\n",
    "print(f'Total rows before removing outliers: {len(drought_df)}')\n",
    "print(f'Total rows after removing outliers: {len(cleaned_drought_df)}')\n",
    "print(f'Number of outliers: {len(drought_df)-len(cleaned_drought_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove outliers\n",
    "\n",
    "# # All cols expect fips\n",
    "# cols = list(soil_df.drop(['fips'], axis=1).columns)\n",
    "\n",
    "# cleaned_soil_df = remove_outliers(soil_df, cols)\n",
    "\n",
    "# # Print the number of rows before and after removing outliers\n",
    "# print(f'Total rows before removing outliers: {len(soil_df)}')\n",
    "# print(f'Total rows after removing outliers: {len(cleaned_soil_df)}')\n",
    "# print(f'Number of outliers: {len(soil_df)-len(cleaned_soil_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET', 'T2M_MAX',\n",
       "       'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
       "       'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'WS50M_RANGE',\n",
       "       'score', 'year', 'month', 'day', 'lat', 'lon', 'elevation', 'slope1',\n",
       "       'slope2', 'slope3', 'slope4', 'slope5', 'slope6', 'slope7', 'slope8',\n",
       "       'aspectN', 'aspectE', 'aspectS', 'aspectW', 'aspectUnknown', 'WAT_LAND',\n",
       "       'NVG_LAND', 'URB_LAND', 'GRS_LAND', 'FOR_LAND', 'CULTRF_LAND',\n",
       "       'CULTIR_LAND', 'CULT_LAND', 'SQ1', 'SQ2', 'SQ3', 'SQ4', 'SQ5', 'SQ6',\n",
       "       'SQ7'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine timeseries and soil data\n",
    "combined_df = cleaned_drought_df.merge(soil_df, how='left', on='fips')\n",
    "\n",
    "# Drop fips code and date\n",
    "combined_df.drop(columns=['fips','date'], inplace=True)\n",
    "\n",
    "# List columns\n",
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape (2443726, 52)\n",
      "Train target shape (2443726,)\n",
      "Test features shape (610932, 52)\n",
      "Test target shape (610932,)\n"
     ]
    }
   ],
   "source": [
    "# Split to X,y train,test\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df.drop(columns=['score']), combined_df['score'], test_size=0.2, random_state=42)   \n",
    "\n",
    "print(\"Train features shape\", X_train.shape)\n",
    "print(\"Train target shape\", y_train.shape)\n",
    "print(\"Test features shape\", X_test.shape)\n",
    "print(\"Test target shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sega9\\AppData\\Local\\Temp\\ipykernel_36212\\1727707840.py:3: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  X_train_ures_SMOTE, y_train_ures_SMOTE = sm.fit_resample(X_train, y_train.ravel())\n"
     ]
    }
   ],
   "source": [
    "# Upsampling using SMOTE\n",
    "sm = SMOTE(random_state = 5)\n",
    "X_train_ures_SMOTE, y_train_ures_SMOTE = sm.fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, the shape of train_X: (2443726, 52)\n",
      "Before OverSampling, the shape of train_y: (2443726,) \n",
      "\n",
      "After OverSampling, the shape of train_X: (9103362, 52)\n",
      "After OverSampling, the shape of train_y: (9103362,) \n",
      "\n",
      "Counts of label '0' - Before Oversampling:1517227, After OverSampling: 1517227\n",
      "Counts of label '1' - Before Oversampling:412980, After OverSampling: 1517227\n",
      "Counts of label '2' - Before Oversampling:249363, After OverSampling: 1517227\n",
      "Counts of label '3' - Before Oversampling:155322, After OverSampling: 1517227\n",
      "Counts of label '4' - Before Oversampling:79514, After OverSampling: 1517227\n",
      "Counts of label '5' - Before Oversampling:29320, After OverSampling: 1517227\n"
     ]
    }
   ],
   "source": [
    "print('Before OverSampling, the shape of train_X: {}'.format(X_train.shape))\n",
    "print('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_ures_SMOTE.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_ures_SMOTE.shape))\n",
    "\n",
    "print(\"Counts of label '0' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 0),sum(y_train_ures_SMOTE == 0)))\n",
    "print(\"Counts of label '1' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 1),sum(y_train_ures_SMOTE == 1)))\n",
    "print(\"Counts of label '2' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 2),sum(y_train_ures_SMOTE == 2)))\n",
    "print(\"Counts of label '3' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 3),sum(y_train_ures_SMOTE == 3)))\n",
    "print(\"Counts of label '4' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 4),sum(y_train_ures_SMOTE == 4)))\n",
    "print(\"Counts of label '5' - Before Oversampling:{}, After OverSampling: {}\".format(sum(y_train == 5),sum(y_train_ures_SMOTE == 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_train_ures_SMOTE_PCAreduced = pca.fit_transform(X_train_ures_SMOTE)\n",
    "X_test_SMOTE_PCA_transformed = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.86640783e-01 1.64789967e-01 9.69241030e-02 8.71608053e-02\n",
      " 7.18999624e-02 4.74643829e-02 3.52313530e-02 2.86831096e-02\n",
      " 2.66474972e-02 2.42013134e-02 2.28222807e-02 1.98126887e-02\n",
      " 1.80508414e-02 1.80282418e-02 1.71054400e-02 1.60981698e-02\n",
      " 1.42803431e-02 1.16296010e-02 1.00580784e-02 9.64310198e-03\n",
      " 8.89189248e-03 8.14791935e-03 7.26556868e-03 7.09963541e-03\n",
      " 6.27328493e-03 5.81834095e-03 5.35582580e-03 3.66526267e-03\n",
      " 3.48718102e-03 2.84125576e-03 2.51774065e-03 2.38704766e-03\n",
      " 2.03092384e-03 1.23370175e-03 1.08161241e-03 7.58772375e-04\n",
      " 7.25047682e-04 6.71654546e-04 6.31249887e-04 6.25187605e-04\n",
      " 5.30179549e-04 4.86321737e-04 1.87302199e-04 9.53610263e-05\n",
      " 1.80462363e-05 1.54791662e-06 4.19116679e-08 2.94164235e-08\n",
      " 1.48756707e-09 1.97428594e-16 4.50340589e-17 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (9103362, 52)\n",
      "y_train Shape: (9103362,)\n",
      "X_test  Shape: (610932, 52)\n",
      "y_test  Shape: (610932,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train Shape: {X_train_ures_SMOTE_PCAreduced.shape}\")\n",
    "print(f\"y_train Shape: {y_train_ures_SMOTE.shape}\")\n",
    "\n",
    "print(f\"X_test  Shape: {X_test_SMOTE_PCA_transformed.shape}\")\n",
    "print(f\"y_test  Shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the objects:\n",
    "with open('data\\Xy_trainTest.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([X_train_ures_SMOTE_PCAreduced, X_test_SMOTE_PCA_transformed, \n",
    "                 y_train_ures_SMOTE, y_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting back the objects:\n",
    "with open('data\\Xy_trainTest.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
