{"cells":[{"cell_type":"markdown","metadata":{},"source":["> ## US Drought & Meteorological Data Starter Notebook\n","This notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors."]},{"cell_type":"markdown","metadata":{},"source":["## Loading the Data\n","In this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes."]},{"cell_type":"markdown","metadata":{},"source":["We load the json files for training, validation and testing into the ``files`` dictionary."]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":false,"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import json\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm.auto import tqdm\n","sns.set_style('white')\n","\n","# files = {}\n","\n","# for dirname, _, filenames in os.walk('data/'):\n","#     for filename in filenames:\n","#         if 'train' in filename:\n","#             files['train'] = os.path.join(dirname, filename)\n","#         if 'valid' in filename:\n","#             files['valid'] = os.path.join(dirname, filename)\n","#         if 'test' in filename:\n","#             files['test'] = os.path.join(dirname, filename)"]},{"cell_type":"markdown","metadata":{},"source":["The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\n","This could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes."]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# class2id = {\n","#     'None': 0,\n","#     'D0': 1,\n","#     'D1': 2,\n","#     'D2': 3,\n","#     'D3': 4,\n","#     'D4': 5,\n","# }\n","# id2class = {v: k for k, v in class2id.items()}"]},{"cell_type":"markdown","metadata":{},"source":["Let's also create a dictionary for the meteorological attributes."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# # Load CSV data for validation\n","# valid_data = pd.read_csv(files['valid'])\n","\n","# # Define attributes mappings\n","# attributes = sorted(valid_data.columns)  # Assuming attributes are column names in the CSV file\n","# id2attr = {i: k for i, k in enumerate(attributes)}\n","# attr2id = {v: k for k, v in id2attr.items()}\n","# attr2id"]},{"cell_type":"markdown","metadata":{},"source":["Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted."]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["# # load one of 'train', 'valid' or 'test'\n","# def loadXY(dataset, shuffle=True, random_state=None):\n","#     data_dict = json.load(open(files[dataset], 'r'))\n","#     keys = sorted(list(data_dict['root'].keys()))\n","    \n","#     if shuffle:\n","#         if random_state is not None:\n","#             np.random.seed(random_state)\n","#         np.random.shuffle(keys)\n","        \n","#     # float16 should be enough and saves some memory\n","#     X = np.zeros([len(keys), 90, 18], dtype=np.float16)\n","#     y = np.zeros([len(keys)], dtype=np.float16)\n","#     # track how many samples are skipped\n","#     skip_count = 0\n","#     for i, key in tqdm(enumerate(keys), total=len(keys), desc=f'loading {dataset} dataset'):\n","#         sample = data_dict['root'][key]\n","#         input_arr = np.zeros([90, 18])\n","#         try:\n","#             for a, j in attr2id.items():\n","#                 input_arr[:,j] = sample['values'][a]\n","#             X[i-skip_count] = input_arr\n","#             y[i-skip_count] = float(class2id[sample['class']])\n","#         except:\n","#             skip_count += 1\n","#     print(f'[{dataset}]: skipped {skip_count} samples ({round(skip_count/len(keys)*100, 3)}%), loaded {len(keys)-skip_count} samples')\n","#     del data_dict\n","#     return X, y"]},{"cell_type":"markdown","metadata":{},"source":["We now load the datasets, this will take a few minutes and use ~8GB of RAM."]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# X_train, y_train = loadXY('train', random_state=42)\n","# X_valid, y_valid = loadXY('valid', random_state=42)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["files = {}\n","for dirname, _, filenames in os.walk('data/'):\n","    for filename in filenames:\n","        if 'train' in filename:\n","            files['train'] = os.path.join(dirname, filename)\n","        if 'valid' in filename:\n","            files['valid'] = os.path.join(dirname, filename)\n","        if 'test' in filename:\n","            files['test'] = os.path.join(dirname, filename)\n","\n","# Define class mappings\n","class2id = {\n","    'None': 0,\n","    'D0': 1,\n","    'D1': 2,\n","    'D2': 3,\n","    'D3': 4,\n","    'D4': 5,\n","}\n","id2class = {v: k for k, v in class2id.items()}\n","\n","# Load CSV data for validation\n","valid_data = pd.read_csv(files['valid'])\n","\n","# Define attributes mappings\n","attributes = sorted(valid_data.columns)  # Assuming attributes are column names in the CSV file\n","id2attr = {i: k for i, k in enumerate(attributes)}\n","attr2id = {v: k for k, v in id2attr.items()}"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["drought_df_train = pd.read_csv('data/train_timeseries.csv')\n","drought_df_test = pd.read_csv('data/test_timeseries.csv')\n","drought_df_val = pd.read_csv('data/validation_timeseries.csv')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["X_train = drought_df_train.drop(columns=['score'],axis=1).to_numpy()\n","y_train = drought_df_train['score'].to_numpy()\n","\n","# X_test = drought_df_test.drop(columns=['score'],axis=1).to_numpy()\n","# y_test = drought_df_test['score'].to_numpy()\n","\n","X_valid = drought_df_val.drop(columns=['score'],axis=1).to_numpy()\n","y_valid = drought_df_val['score'].to_numpy()"]},{"cell_type":"markdown","metadata":{},"source":["## LSTM\n","Let's train a simple LSTM on the data, treating this as a regression problem."]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing and Loading"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["batch_size_factor = 3\n","batch_size = 256 * batch_size_factor"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"ename":"InvalidIndexError","evalue":"(slice(None, None, None), slice(None, None, None), 0)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:173\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), slice(None, None, None), 0)' is an invalid key","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m scaler_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr_id \u001b[38;5;129;01min\u001b[39;00m id2attr\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      6\u001b[0m     scaler_dict[attr_id] \u001b[38;5;241m=\u001b[39m RobustScaler()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m----> 7\u001b[0m         \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattr_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m     X_train[:,:,attr_id] \u001b[38;5;241m=\u001b[39m scaler_dict[attr_id]\u001b[38;5;241m.\u001b[39mtransform(X_train[:,:,attr_id]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m90\u001b[39m)\n","File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3817\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3817\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6059\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   6055\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   6056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   6057\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   6058\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 6059\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n","\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), slice(None, None, None), 0)"]}],"source":["from sklearn.preprocessing import RobustScaler\n","\n","scaler_dict = {}\n","\n","for attr_id in id2attr.keys():\n","    scaler_dict[attr_id] = RobustScaler().fit(\n","        X_train[:,:,attr_id].reshape(-1, 1)\n","    )\n","    X_train[:,:,attr_id] = scaler_dict[attr_id].transform(X_train[:,:,attr_id].reshape(-1, 1)).reshape(-1, 90)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for attr_id in id2attr.keys():\n","    X_valid[:,:,attr_id] = scaler_dict[attr_id].transform(X_valid[:,:,attr_id].reshape(-1, 1)).reshape(-1, 90)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","train_data = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n","train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["valid_data = TensorDataset(torch.tensor(X_valid), torch.tensor(y_valid))\n","valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, drop_last=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# hyper parameters\n","lr = 7e-5 * batch_size_factor\n","output_size = 1\n","hidden_dim = 512\n","dropout = 0.1\n","n_layers = 4\n","epochs = 10\n","clip = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","class DroughtNetLSTM(nn.Module):\n","    def __init__(self, output_size, num_input_features, hidden_dim, n_layers, drop_prob=0.2):\n","        super(DroughtNetLSTM, self).__init__()\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        self.lstm = nn.LSTM(num_input_features, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        self.dropout = nn.Dropout(drop_prob)\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        \n","    def forward(self, x, hidden):\n","        batch_size = x.size(0)\n","        x = x.cuda().to(dtype=torch.float32)\n","        lstm_out, hidden = self.lstm(x, hidden)\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        \n","        out = out.view(batch_size, -1)\n","        out = out[:,-1]\n","        return out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        hidden = (\n","            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n","            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n","        )\n","        return hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n","is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print('using GPU')\n","else:\n","    device = torch.device(\"cpu\")\n","    print('using CPU')\n","\n","\n","model = DroughtNetLSTM(output_size, len(id2attr), hidden_dim, n_layers, dropout)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loss_function = nn.MSELoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)\n","scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","counter = 0\n","valid_loss_min = np.Inf\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","model.train()\n","\n","for i in range(epochs):\n","    h = model.init_hidden(batch_size)\n","    \n","    for k, (inputs, labels) in tqdm(enumerate(train_loader), desc=f'epoch {i+1}/{epochs}', total=len(train_loader)):\n","        counter += 1\n","        h = tuple([e.data for e in h])\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        model.zero_grad()\n","        output, h = model(inputs, h)\n","        loss = loss_function(output.squeeze(), labels.float())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n","            val_h = model.init_hidden(batch_size)\n","            val_losses = []\n","            model.eval()\n","            labels = []\n","            preds = []\n","            for inp, lab in valid_loader:\n","                val_h = tuple([each.data for each in val_h])\n","                inp, lab = inp.to(device), lab.to(device)\n","                out, val_h = model(inp, val_h)\n","                val_loss = loss_function(out.squeeze(), lab.float())\n","                val_losses.append(val_loss.item())\n","                for l in lab:\n","                    labels.append(int(l))\n","                for p in out.round():\n","                    if p > 5:\n","                        p = 5\n","                    if p < 0:\n","                        p = 0\n","                    preds.append(int(p))\n","            \n","            # log data\n","            log_dict = {\n","                'loss': float(loss),\n","                'epoch': counter/len(train_loader),\n","                'step': counter,\n","                'lr': scheduler.get_last_lr()[0]\n","            }\n","            log_dict['validation_loss'] = np.mean(val_losses)\n","            log_dict[f'macro_f1'] = f1_score(labels, preds, average='macro')\n","            log_dict[f'micro_f1'] = f1_score(labels, preds, average='micro')\n","            for j, f1 in enumerate(f1_score(labels, preds, average=None)):\n","                log_dict[f'{id2class[j]}_f1'] = f1\n","            print(log_dict)\n","            \n","            model.train()\n","            \n","            if np.mean(val_losses) <= valid_loss_min:\n","                torch.save(model.state_dict(), './state_dict.pt')\n","                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n","                valid_loss_min = np.mean(val_losses)"]},{"cell_type":"markdown","metadata":{},"source":["Best Macro F1 - **0.304**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
