{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn libraries for metrics\n",
    "from sklearn.metrics import f1_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device, use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network classifier for drought prediction.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input features.\n",
    "        hidden_sizes (list of int): A list containing the sizes of the hidden layers.\n",
    "        output_size (int): The number of output classes.\n",
    "        dropout_prob (float, optional): The probability of an element to be zeroed in dropout. Default is 0.5.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): A list of linear layers.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_prob=0.5):\n",
    "        super(DroughtClassifier, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        # Apply each layer followed by ReLU activation and dropout, except the last layer\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.dropout(F.relu(layer(x)))\n",
    "        # Apply the last layer without activation or dropout\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and prints the test loss, accuracy, Macro F1 Mean, and MAE Mean.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained neural network model to be evaluated.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (nn.Module): The loss function.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():                                           # Disable gradient computation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)   # Move inputs and labels to GPU\n",
    "\n",
    "            outputs = model(inputs)             # Forward pass\n",
    "            loss = criterion(outputs, labels)   # Compute loss\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)    # Accumulate loss\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)                            # Get predictions\n",
    "            correct_predictions += torch.sum(preds == labels).item()    # Count correct predictions\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())                     # Collect all labels\n",
    "            all_preds.extend(preds.cpu().numpy())                       # Collect all predictions\n",
    "\n",
    "    test_loss = running_loss / len(test_loader.dataset)         # Calculate average test loss\n",
    "    accuracy = correct_predictions / len(test_loader.dataset)   # Calculate test accuracy\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro') # Calculate Macro F1 Mean\n",
    "    mae = mean_absolute_error(all_labels, all_preds)            # Calculate MAE Mean\n",
    "    \n",
    "    # Print test metrics\n",
    "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, Macro F1 Mean: {macro_f1:.4f}, MAE Mean: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_for_model(input_series, scaler_file='data/scaler.pkl', pca_model_file='data/pca_model.pkl'):\n",
    "    # Step 1: Merge input_series with soil_df based on 'fips'\n",
    "    soil_df = pd.read_csv('data/soil_data.csv')\n",
    "    input_data = pd.DataFrame(input_series).T.merge(soil_df, on='fips', how='left')\n",
    "    \n",
    "    # Step 2: Drop unnecessary columns 'date' and 'fips' if they exist\n",
    "    input_data.drop(columns=['date', 'fips'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Step 3: Load the saved StandardScaler object\n",
    "    with open(scaler_file, 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    \n",
    "    # Step 4: Scale the input data\n",
    "    scaled_data = scaler.transform(input_data)\n",
    "    \n",
    "    # Step 5: Load the saved PCA object and apply transformation\n",
    "    with open(pca_model_file, 'rb') as file:\n",
    "        pca_model = pickle.load(file)\n",
    "    \n",
    "    pca_transformed = pca_model.transform(scaled_data)\n",
    "    \n",
    "    return pca_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format input and make predictions\n",
    "def predict_formatted_input(model, input_series):\n",
    "    # Step 1: Format input for model\n",
    "    formatted_input = format_input_for_model(input_series)\n",
    "    \n",
    "    # Step 2: Convert formatted_input to torch tensor\n",
    "    input_tensor = torch.tensor(formatted_input, dtype=torch.float32)\n",
    "    \n",
    "    # Step 3: Ensure model is in evaluation mode and on CPU\n",
    "    model.eval()\n",
    "    model.cpu()  # Move model to CPU explicitly\n",
    "    \n",
    "    # Step 4: Move input_tensor to CPU if it's not already\n",
    "    input_tensor = input_tensor.cpu()\n",
    "    \n",
    "    # Step 5: Perform prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)  # Assuming classification task, get predicted class index\n",
    "    \n",
    "    return predicted.item()  # Return the predicted class as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retrained model\n",
    "with open('saved_models//retrained_model_stepLR2.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "\n",
    "# Load training and testing data from a pickle file\n",
    "with open('data/Xy_trainTest.pkl', 'rb') as f:\n",
    "    # Unpickle the data into training and testing datasets\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6352, Accuracy: 0.7337, Macro F1 Mean: 0.6895, MAE Mean: 0.3255\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Series:\n",
      "fips                 1001\n",
      "date           2000-01-11\n",
      "PRECTOT              1.33\n",
      "PS                  100.4\n",
      "QV2M                 6.63\n",
      "T2M                 11.48\n",
      "T2MDEW               7.84\n",
      "T2MWET               7.84\n",
      "T2M_MAX             18.88\n",
      "T2M_MIN              5.72\n",
      "T2M_RANGE           13.16\n",
      "TS                  10.43\n",
      "WS10M                1.76\n",
      "WS10M_MAX            2.48\n",
      "WS10M_MIN            1.05\n",
      "WS10M_RANGE          1.43\n",
      "WS50M                3.55\n",
      "WS50M_MAX            6.38\n",
      "WS50M_MIN            1.71\n",
      "WS50M_RANGE          4.67\n",
      "year                 2000\n",
      "month                   1\n",
      "day                    11\n",
      "Name: 1, dtype: object\n",
      "\n",
      "Predicted Class: 2\n",
      "True Class     : 2\n"
     ]
    }
   ],
   "source": [
    "row = 1\n",
    "\n",
    "input_series = drought_df.drop(columns=['score']).iloc[row]\n",
    "true_score = drought_df['score'].iloc[row]\n",
    "\n",
    "# Predict with the model\n",
    "predicted_class = predict_formatted_input(model, input_series)\n",
    "\n",
    "# Display input_series and prediction\n",
    "print(f\"Input Series:\\n{input_series}\")\n",
    "print(f\"\\nPredicted Class: {predicted_class}\")\n",
    "print(f\"True Class     : {true_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
