{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "\n",
    "with open('data\\Xy_trainTest.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture: It has one input layer, two hidden layers (both with the same number of neurons), and one output layer.  \n",
    "Activation Function: ReLU activation function is applied after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class DroughtNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DroughtNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = len(set(y_train.tolist()))\n",
    "\n",
    "model = DroughtNet(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9895, Accuracy: 0.6303, Validation Loss: 0.9396, Validation Accuracy: 0.6417\n",
      "Epoch 2/3, Loss: 0.9193, Accuracy: 0.6458, Validation Loss: 0.9040, Validation Accuracy: 0.6497\n",
      "Epoch 3/3, Loss: 0.8957, Accuracy: 0.6515, Validation Loss: 0.8872, Validation Accuracy: 0.6538\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 3\n",
    "\n",
    "train_history = {'loss': [], 'accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_history['loss'].append(epoch_loss)\n",
    "    train_history['accuracy'].append(epoch_accuracy)\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            outputs = model(X_val)\n",
    "            val_loss = criterion(outputs, y_val)\n",
    "            val_running_loss += val_loss.item() * X_val.size(0)\n",
    "            \n",
    "            _, val_predicted = torch.max(outputs, 1)\n",
    "            val_correct += (val_predicted == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "    val_epoch_accuracy = val_correct / val_total\n",
    "    train_history['validation_loss'].append(val_epoch_loss)\n",
    "    train_history['validation_accuracy'].append(val_epoch_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model and associated data\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_history': train_history,\n",
    "}, 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved model\n",
    "# checkpoint = torch.load('saved_model.pth')\n",
    "\n",
    "# # Load the optimizer state\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# # Retrieve training history\n",
    "# train_history = checkpoint['train_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred_list.append(y_pred.numpy())\n",
    "\n",
    "# Flatten the list of predictions\n",
    "y_pred = np.concatenate(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_test: [0 1 2 3 4 5]\n",
      "Unique values in y_pred: [0 1 2 3 4 5]\n",
      "Shape of y_test: (680652,)\n",
      "Shape of y_pred: (680652,)\n",
      "Accuracy: 0.6538480750809518\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83    419153\n",
      "           1       0.34      0.15      0.21    115637\n",
      "           2       0.32      0.15      0.20     70070\n",
      "           3       0.32      0.27      0.29     44252\n",
      "           4       0.38      0.25      0.30     23193\n",
      "           5       0.43      0.33      0.37      8347\n",
      "\n",
      "    accuracy                           0.65    680652\n",
      "   macro avg       0.42      0.35      0.37    680652\n",
      "weighted avg       0.58      0.65      0.60    680652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test to numpy array if not already\n",
    "y_test_numpy = y_test.to_numpy() if not isinstance(y_test, np.ndarray) else y_test\n",
    "\n",
    "# Convert y_pred to numpy array\n",
    "y_pred_numpy = np.concatenate(y_pred_list)\n",
    "\n",
    "# Check the unique values in both y_test and y_pred\n",
    "print(f\"Unique values in y_test: {np.unique(y_test_numpy)}\")\n",
    "print(f\"Unique values in y_pred: {np.unique(y_pred_numpy)}\")\n",
    "\n",
    "# Ensure y_pred is a 1D array\n",
    "y_pred_flat = y_pred_numpy.flatten()\n",
    "\n",
    "# Check the shapes to ensure they match\n",
    "print(f\"Shape of y_test: {y_test_numpy.shape}\")\n",
    "print(f\"Shape of y_pred: {y_pred_flat.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_numpy, y_pred_flat)\n",
    "report = classification_report(y_test_numpy, y_pred_flat)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "# make_dot(model(X_train_tensor), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture: It has one input layer, a variable number of hidden layers (as specified by the hidden_dims list), and one output layer.  \n",
    "Activation Function: ReLU activation function is applied after each hidden layer.  \n",
    "Flexibility: High, as the number and size of hidden layers can be adjusted by passing different hidden_dims lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtNetComplex(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(DroughtNetComplex, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims)-1)])\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            out = self.relu(layer(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions for hidden layers\n",
    "hidden_dims = [256, 128, 64]  # Example: Three hidden layers with 256, 128, and 64 neurons respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with increased complexity\n",
    "model_complex = DroughtNetComplex(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "# Define the optimizer and criterion (loss function)\n",
    "optimizer_complex = optim.Adam(model_complex.parameters(), lr=0.001)\n",
    "criterion_complex = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9778, Accuracy: 0.6325, Validation Loss: 0.9253, Validation Accuracy: 0.6434\n",
      "Epoch 2/3, Loss: 0.9115, Accuracy: 0.6467, Validation Loss: 0.9024, Validation Accuracy: 0.6489\n",
      "Epoch 3/3, Loss: 0.8910, Accuracy: 0.6516, Validation Loss: 0.8805, Validation Accuracy: 0.6545\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [128, 64, 32]  # Example hidden dimensions, you can adjust as needed\n",
    "output_dim = len(set(y_train.tolist()))\n",
    "\n",
    "model = DroughtNetComplex(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 3\n",
    "\n",
    "train_history = {'loss': [], 'accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_history['loss'].append(epoch_loss)\n",
    "    train_history['accuracy'].append(epoch_accuracy)\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            outputs = model(X_val)\n",
    "            val_loss = criterion(outputs, y_val)\n",
    "            val_running_loss += val_loss.item() * X_val.size(0)\n",
    "            \n",
    "            _, val_predicted = torch.max(outputs, 1)\n",
    "            val_correct += (val_predicted == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "    val_epoch_accuracy = val_correct / val_total\n",
    "    train_history['validation_loss'].append(val_epoch_loss)\n",
    "    train_history['validation_accuracy'].append(val_epoch_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model and associated data\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_history': train_history,\n",
    "}, 'saved_model2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the complex model\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred_list.append(y_pred.numpy())\n",
    "\n",
    "# Flatten the list of predictions\n",
    "y_pred = np.concatenate(y_pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_test: [0 1 2 3 4 5]\n",
      "Unique values in y_pred: [0 1 2 3 4 5]\n",
      "Shape of y_test: (680652,)\n",
      "Shape of y_pred: (680652,)\n",
      "Accuracy: 0.6544754147493873\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.95      0.83    419153\n",
      "           1       0.34      0.14      0.20    115637\n",
      "           2       0.32      0.17      0.22     70070\n",
      "           3       0.35      0.20      0.26     44252\n",
      "           4       0.34      0.31      0.32     23193\n",
      "           5       0.45      0.29      0.35      8347\n",
      "\n",
      "    accuracy                           0.65    680652\n",
      "   macro avg       0.42      0.34      0.36    680652\n",
      "weighted avg       0.58      0.65      0.60    680652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test to numpy array if not already\n",
    "y_test_numpy = y_test.to_numpy() if not isinstance(y_test, np.ndarray) else y_test\n",
    "\n",
    "# Convert y_pred to numpy array\n",
    "y_pred_numpy = np.concatenate(y_pred_list)\n",
    "\n",
    "# Check the unique values in both y_test and y_pred\n",
    "print(f\"Unique values in y_test: {np.unique(y_test_numpy)}\")\n",
    "print(f\"Unique values in y_pred: {np.unique(y_pred_numpy)}\")\n",
    "\n",
    "# Ensure y_pred is a 1D array\n",
    "y_pred_flat = y_pred_numpy.flatten()\n",
    "\n",
    "# Check the shapes to ensure they match\n",
    "print(f\"Shape of y_test: {y_test_numpy.shape}\")\n",
    "print(f\"Shape of y_pred: {y_pred_flat.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_numpy, y_pred_flat)\n",
    "report = classification_report(y_test_numpy, y_pred_flat)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search Space for hidden layer size/amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define a function to create the model with variable hidden dimensions\n",
    "def create_model(input_dim, hidden_dims, output_dim):\n",
    "    return DroughtNetComplex(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "# Define a function to train the model\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    train_history = {'loss': [], 'accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        train_history['loss'].append(epoch_loss)\n",
    "        train_history['accuracy'].append(epoch_accuracy)\n",
    "        \n",
    "        # Calculate validation loss and accuracy\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in test_loader:\n",
    "                outputs = model(X_val)\n",
    "                val_loss = criterion(outputs, y_val)\n",
    "                val_running_loss += val_loss.item() * X_val.size(0)\n",
    "                \n",
    "                _, val_predicted = torch.max(outputs, 1)\n",
    "                val_correct += (val_predicted == y_val).sum().item()\n",
    "                val_total += y_val.size(0)\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "        val_epoch_accuracy = val_correct / val_total\n",
    "        train_history['validation_loss'].append(val_epoch_loss)\n",
    "        train_history['validation_accuracy'].append(val_epoch_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
    "    \n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hidden_layer_choices(num_choices, min_layers, max_layers, min_neurons, max_neurons):\n",
    "    hidden_layer_choices = []\n",
    "    \n",
    "    for _ in range(num_choices):\n",
    "        # Randomly choose the number of layers\n",
    "        num_layers = random.randint(min_layers, max_layers)\n",
    "        \n",
    "        # Randomly choose the number of neurons for each layer\n",
    "        layers = [random.randint(min_neurons, max_neurons) for _ in range(num_layers)]\n",
    "        \n",
    "        hidden_layer_choices.append(layers)\n",
    "    \n",
    "    return hidden_layer_choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter random search space: [[989, 614, 167], [36, 796, 477], [397, 324], [881, 626, 379], [672], [313, 985, 401], [891, 405], [203, 752], [325, 167, 998], [738, 881, 242], [425, 661, 526], [937], [746, 232, 707, 249], [228], [694, 924], [574, 484, 564, 146, 820], [522, 470], [502, 171], [554, 557], [65, 786, 148, 786, 489], [536, 155, 544, 413], [636, 395, 585, 285], [595, 202], [947, 620, 958, 51, 328], [994, 600, 985, 752], [580], [1021, 348, 253], [474], [394, 895, 600, 702], [497, 149, 282, 507], [53, 755, 595, 418, 900], [406, 273, 784, 936, 380], [629], [227, 508], [704, 617, 199], [793, 901, 57, 244, 353], [127, 500, 552], [810, 485, 356], [696, 917, 18, 642, 596], [504, 291, 191, 1014, 454], [264, 210, 269, 567], [824, 895, 19, 901, 815], [296, 70, 1019], [241], [287, 440], [742, 859], [633, 166, 677], [192], [204, 826, 224, 892, 81], [602, 584], [66, 434, 436, 91], [82], [1022], [735, 815], [599, 290, 131, 808], [75, 180], [361, 699, 446], [208, 410, 273, 673], [542, 955, 366], [320, 505, 417], [277, 542, 926, 154, 700], [69], [52, 755, 432, 535], [1004, 422, 635, 465], [886, 82, 223, 546, 82], [784, 1017, 41], [721, 216, 475, 756], [995, 453, 105], [472, 711, 129], [37, 830, 426], [600, 409], [530, 624, 601, 325], [398, 744, 558, 835, 438], [385], [839, 995, 231, 805, 518], [88, 22, 469, 1018, 596], [507], [229, 361, 126, 408], [470, 126, 229], [568], [869, 528, 553], [16], [547, 521, 720, 784], [505, 122, 15], [162, 762], [291, 105], [1011, 391, 924, 29, 571], [481, 365], [654, 522, 472, 490, 560], [954], [277, 25, 467, 496, 675], [397, 259, 31], [223, 584, 291], [642, 436], [198, 53, 632], [656, 411], [818, 467, 852, 336], [651], [667, 930], [260, 130, 448]]\n",
      "Random Search 1/10\n",
      "Epoch 1/3, Loss: 0.9328, Accuracy: 0.6436, Validation Loss: 0.8589, Validation Accuracy: 0.6623\n",
      "Epoch 2/3, Loss: 0.8305, Accuracy: 0.6708, Validation Loss: 0.8056, Validation Accuracy: 0.6789\n",
      "Epoch 3/3, Loss: 0.7872, Accuracy: 0.6851, Validation Loss: 0.7696, Validation Accuracy: 0.6905\n",
      "Random Search 2/10\n",
      "Epoch 1/3, Loss: 0.9421, Accuracy: 0.6411, Validation Loss: 0.8760, Validation Accuracy: 0.6569\n",
      "Epoch 2/3, Loss: 0.8459, Accuracy: 0.6658, Validation Loss: 0.8287, Validation Accuracy: 0.6712\n",
      "Epoch 3/3, Loss: 0.8101, Accuracy: 0.6775, Validation Loss: 0.8020, Validation Accuracy: 0.6799\n",
      "Random Search 3/10\n",
      "Epoch 1/3, Loss: 0.9408, Accuracy: 0.6415, Validation Loss: 0.8812, Validation Accuracy: 0.6552\n",
      "Epoch 2/3, Loss: 0.8487, Accuracy: 0.6652, Validation Loss: 0.8218, Validation Accuracy: 0.6744\n",
      "Epoch 3/3, Loss: 0.8123, Accuracy: 0.6767, Validation Loss: 0.8016, Validation Accuracy: 0.6795\n",
      "Random Search 4/10\n",
      "Epoch 1/3, Loss: 0.9803, Accuracy: 0.6313, Validation Loss: 0.9330, Validation Accuracy: 0.6409\n",
      "Epoch 2/3, Loss: 0.9161, Accuracy: 0.6443, Validation Loss: 0.8948, Validation Accuracy: 0.6491\n",
      "Epoch 3/3, Loss: 0.8898, Accuracy: 0.6508, Validation Loss: 0.8829, Validation Accuracy: 0.6547\n",
      "Random Search 5/10\n",
      "Epoch 1/3, Loss: 1.0299, Accuracy: 0.6219, Validation Loss: 0.9998, Validation Accuracy: 0.6280\n",
      "Epoch 2/3, Loss: 0.9853, Accuracy: 0.6309, Validation Loss: 0.9699, Validation Accuracy: 0.6348\n",
      "Epoch 3/3, Loss: 0.9657, Accuracy: 0.6352, Validation Loss: 0.9623, Validation Accuracy: 0.6346\n",
      "Random Search 6/10\n",
      "Epoch 1/3, Loss: 0.9595, Accuracy: 0.6351, Validation Loss: 0.9025, Validation Accuracy: 0.6474\n",
      "Epoch 2/3, Loss: 0.8811, Accuracy: 0.6529, Validation Loss: 0.8576, Validation Accuracy: 0.6592\n",
      "Epoch 3/3, Loss: 0.8482, Accuracy: 0.6628, Validation Loss: 0.8381, Validation Accuracy: 0.6662\n",
      "Random Search 7/10\n",
      "Epoch 1/3, Loss: 0.9481, Accuracy: 0.6388, Validation Loss: 0.8813, Validation Accuracy: 0.6526\n",
      "Epoch 2/3, Loss: 0.8505, Accuracy: 0.6629, Validation Loss: 0.8272, Validation Accuracy: 0.6706\n",
      "Epoch 3/3, Loss: 0.8091, Accuracy: 0.6754, Validation Loss: 0.8068, Validation Accuracy: 0.6759\n",
      "Random Search 8/10\n",
      "Epoch 1/3, Loss: 0.9348, Accuracy: 0.6427, Validation Loss: 0.8611, Validation Accuracy: 0.6620\n",
      "Epoch 2/3, Loss: 0.8271, Accuracy: 0.6711, Validation Loss: 0.8039, Validation Accuracy: 0.6787\n",
      "Epoch 3/3, Loss: 0.7845, Accuracy: 0.6846, Validation Loss: 0.7743, Validation Accuracy: 0.6880\n",
      "Random Search 9/10\n",
      "Epoch 1/3, Loss: 0.9373, Accuracy: 0.6424, Validation Loss: 0.8722, Validation Accuracy: 0.6581\n",
      "Epoch 2/3, Loss: 0.8398, Accuracy: 0.6672, Validation Loss: 0.8152, Validation Accuracy: 0.6748\n",
      "Epoch 3/3, Loss: 0.7979, Accuracy: 0.6811, Validation Loss: 0.7883, Validation Accuracy: 0.6843\n",
      "Random Search 10/10\n",
      "Epoch 1/3, Loss: 0.9448, Accuracy: 0.6399, Validation Loss: 0.8742, Validation Accuracy: 0.6570\n",
      "Epoch 2/3, Loss: 0.8492, Accuracy: 0.6641, Validation Loss: 0.8270, Validation Accuracy: 0.6709\n",
      "Epoch 3/3, Loss: 0.8126, Accuracy: 0.6754, Validation Loss: 0.8097, Validation Accuracy: 0.6775\n",
      "Best validation accuracy: 0.6904938206308069\n",
      "Best hidden dimensions: [223, 584, 291]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for hidden layer generation\n",
    "num_choices = 100  # Number of different hidden layer configurations to generate\n",
    "min_layers = 1    # Minimum number of hidden layers\n",
    "max_layers = 5    # Maximum number of hidden layers\n",
    "min_neurons = 8  # Minimum number of neurons in each hidden layer\n",
    "max_neurons = 1024 # Maximum number of neurons in each hidden layer\n",
    "\n",
    "# Generate hidden layer choices\n",
    "hidden_layer_choices = generate_hidden_layer_choices(num_choices, min_layers, max_layers, min_neurons, max_neurons)\n",
    "print(f'Hyperparameter random search space: {hidden_layer_choices}')\n",
    "\n",
    "# Number of random searches\n",
    "num_searches = 10\n",
    "\n",
    "# Store the best model and its performance\n",
    "best_model = None\n",
    "best_val_accuracy = 0\n",
    "best_hidden_dims = None\n",
    "\n",
    "for i in range(num_searches):\n",
    "    print(f\"Random Search {i+1}/{num_searches}\")\n",
    "    hidden_dims = random.choice(hidden_layer_choices)\n",
    "    model = create_model(input_dim, hidden_dims, output_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_history = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "    \n",
    "    # Get the validation accuracy of the final epoch\n",
    "    val_accuracy = train_history['validation_accuracy'][-1]\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "        best_hidden_dims = hidden_dims\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_accuracy}\")\n",
    "print(f\"Best hidden dimensions: {best_hidden_dims}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9320, Accuracy: 0.6439, Validation Loss: 0.8609, Validation Accuracy: 0.6618\n",
      "Epoch 2/10, Loss: 0.8298, Accuracy: 0.6712, Validation Loss: 0.8085, Validation Accuracy: 0.6777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(input_dim, best_hidden_dims, output_dim)\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m train_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[34], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     22\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sega9\\anaconda3\\envs\\py311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train with best hidden dimensions for more Epochs\n",
    "\n",
    "model = create_model(input_dim, best_hidden_dims, output_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_history = train_model(model, train_loader, test_loader, criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
