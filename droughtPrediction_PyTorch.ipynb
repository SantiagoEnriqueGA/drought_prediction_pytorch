{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drought Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_df =  pd.read_csv('data/all_timeseries.csv')\n",
    "\n",
    "with open('data\\Xy_trainTest.pkl', 'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture: It has one input layer, two hidden layers (both with the same number of neurons), and one output layer.  \n",
    "Activation Function: ReLU activation function is applied after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class DroughtNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DroughtNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = len(set(y_train.tolist()))\n",
    "\n",
    "model = DroughtNet(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9895, Accuracy: 0.6303, Validation Loss: 0.9396, Validation Accuracy: 0.6417\n",
      "Epoch 2/3, Loss: 0.9193, Accuracy: 0.6458, Validation Loss: 0.9040, Validation Accuracy: 0.6497\n",
      "Epoch 3/3, Loss: 0.8957, Accuracy: 0.6515, Validation Loss: 0.8872, Validation Accuracy: 0.6538\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 3\n",
    "\n",
    "train_history = {'loss': [], 'accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_history['loss'].append(epoch_loss)\n",
    "    train_history['accuracy'].append(epoch_accuracy)\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            outputs = model(X_val)\n",
    "            val_loss = criterion(outputs, y_val)\n",
    "            val_running_loss += val_loss.item() * X_val.size(0)\n",
    "            \n",
    "            _, val_predicted = torch.max(outputs, 1)\n",
    "            val_correct += (val_predicted == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "    val_epoch_accuracy = val_correct / val_total\n",
    "    train_history['validation_loss'].append(val_epoch_loss)\n",
    "    train_history['validation_accuracy'].append(val_epoch_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model and associated data\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_history': train_history,\n",
    "}, 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved model\n",
    "# checkpoint = torch.load('saved_model.pth')\n",
    "\n",
    "# # Load the optimizer state\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# # Retrieve training history\n",
    "# train_history = checkpoint['train_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred_list.append(y_pred.numpy())\n",
    "\n",
    "# Flatten the list of predictions\n",
    "y_pred = np.concatenate(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_test: [0 1 2 3 4 5]\n",
      "Unique values in y_pred: [0 1 2 3 4 5]\n",
      "Shape of y_test: (680652,)\n",
      "Shape of y_pred: (680652,)\n",
      "Accuracy: 0.6538480750809518\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83    419153\n",
      "           1       0.34      0.15      0.21    115637\n",
      "           2       0.32      0.15      0.20     70070\n",
      "           3       0.32      0.27      0.29     44252\n",
      "           4       0.38      0.25      0.30     23193\n",
      "           5       0.43      0.33      0.37      8347\n",
      "\n",
      "    accuracy                           0.65    680652\n",
      "   macro avg       0.42      0.35      0.37    680652\n",
      "weighted avg       0.58      0.65      0.60    680652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test to numpy array if not already\n",
    "y_test_numpy = y_test.to_numpy() if not isinstance(y_test, np.ndarray) else y_test\n",
    "\n",
    "# Convert y_pred to numpy array\n",
    "y_pred_numpy = np.concatenate(y_pred_list)\n",
    "\n",
    "# Check the unique values in both y_test and y_pred\n",
    "print(f\"Unique values in y_test: {np.unique(y_test_numpy)}\")\n",
    "print(f\"Unique values in y_pred: {np.unique(y_pred_numpy)}\")\n",
    "\n",
    "# Ensure y_pred is a 1D array\n",
    "y_pred_flat = y_pred_numpy.flatten()\n",
    "\n",
    "# Check the shapes to ensure they match\n",
    "print(f\"Shape of y_test: {y_test_numpy.shape}\")\n",
    "print(f\"Shape of y_pred: {y_pred_flat.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_numpy, y_pred_flat)\n",
    "report = classification_report(y_test_numpy, y_pred_flat)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "# make_dot(model(X_train_tensor), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture: It has one input layer, a variable number of hidden layers (as specified by the hidden_dims list), and one output layer.  \n",
    "Activation Function: ReLU activation function is applied after each hidden layer.  \n",
    "Flexibility: High, as the number and size of hidden layers can be adjusted by passing different hidden_dims lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtNetComplex(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(DroughtNetComplex, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims)-1)])\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            out = self.relu(layer(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions for hidden layers\n",
    "hidden_dims = [256, 128, 64]  # Example: Three hidden layers with 256, 128, and 64 neurons respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with increased complexity\n",
    "model_complex = DroughtNetComplex(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "# Define the optimizer and criterion (loss function)\n",
    "optimizer_complex = optim.Adam(model_complex.parameters(), lr=0.001)\n",
    "criterion_complex = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.9778, Accuracy: 0.6325, Validation Loss: 0.9253, Validation Accuracy: 0.6434\n",
      "Epoch 2/3, Loss: 0.9115, Accuracy: 0.6467, Validation Loss: 0.9024, Validation Accuracy: 0.6489\n",
      "Epoch 3/3, Loss: 0.8910, Accuracy: 0.6516, Validation Loss: 0.8805, Validation Accuracy: 0.6545\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dims = [128, 64, 32]  # Example hidden dimensions, you can adjust as needed\n",
    "output_dim = len(set(y_train.tolist()))\n",
    "\n",
    "model = DroughtNetComplex(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 3\n",
    "\n",
    "train_history = {'loss': [], 'accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_history['loss'].append(epoch_loss)\n",
    "    train_history['accuracy'].append(epoch_accuracy)\n",
    "    \n",
    "    # Calculate validation loss and accuracy\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in test_loader:\n",
    "            outputs = model(X_val)\n",
    "            val_loss = criterion(outputs, y_val)\n",
    "            val_running_loss += val_loss.item() * X_val.size(0)\n",
    "            \n",
    "            _, val_predicted = torch.max(outputs, 1)\n",
    "            val_correct += (val_predicted == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "    val_epoch_accuracy = val_correct / val_total\n",
    "    train_history['validation_loss'].append(val_epoch_loss)\n",
    "    train_history['validation_accuracy'].append(val_epoch_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
    "\n",
    "# Save the trained model and associated data\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_history': train_history,\n",
    "}, 'saved_model2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the complex model\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, y_pred = torch.max(outputs, 1)\n",
    "        y_pred_list.append(y_pred.numpy())\n",
    "\n",
    "# Flatten the list of predictions\n",
    "y_pred = np.concatenate(y_pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_test: [0 1 2 3 4 5]\n",
      "Unique values in y_pred: [0 1 2 3 4 5]\n",
      "Shape of y_test: (680652,)\n",
      "Shape of y_pred: (680652,)\n",
      "Accuracy: 0.6544754147493873\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.95      0.83    419153\n",
      "           1       0.34      0.14      0.20    115637\n",
      "           2       0.32      0.17      0.22     70070\n",
      "           3       0.35      0.20      0.26     44252\n",
      "           4       0.34      0.31      0.32     23193\n",
      "           5       0.45      0.29      0.35      8347\n",
      "\n",
      "    accuracy                           0.65    680652\n",
      "   macro avg       0.42      0.34      0.36    680652\n",
      "weighted avg       0.58      0.65      0.60    680652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test to numpy array if not already\n",
    "y_test_numpy = y_test.to_numpy() if not isinstance(y_test, np.ndarray) else y_test\n",
    "\n",
    "# Convert y_pred to numpy array\n",
    "y_pred_numpy = np.concatenate(y_pred_list)\n",
    "\n",
    "# Check the unique values in both y_test and y_pred\n",
    "print(f\"Unique values in y_test: {np.unique(y_test_numpy)}\")\n",
    "print(f\"Unique values in y_pred: {np.unique(y_pred_numpy)}\")\n",
    "\n",
    "# Ensure y_pred is a 1D array\n",
    "y_pred_flat = y_pred_numpy.flatten()\n",
    "\n",
    "# Check the shapes to ensure they match\n",
    "print(f\"Shape of y_test: {y_test_numpy.shape}\")\n",
    "print(f\"Shape of y_pred: {y_pred_flat.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test_numpy, y_pred_flat)\n",
    "report = classification_report(y_test_numpy, y_pred_flat)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Define a function to create the model with variable hidden dimensions\n",
    "def create_model(input_dim, hidden_dims, output_dim):\n",
    "    return DroughtNetComplex(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "# Define a function to train the model\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    train_history = {'loss': [], 'accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_accuracy = correct / total\n",
    "        train_history['loss'].append(epoch_loss)\n",
    "        train_history['accuracy'].append(epoch_accuracy)\n",
    "        \n",
    "        # Calculate validation loss and accuracy\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in test_loader:\n",
    "                outputs = model(X_val)\n",
    "                val_loss = criterion(outputs, y_val)\n",
    "                val_running_loss += val_loss.item() * X_val.size(0)\n",
    "                \n",
    "                _, val_predicted = torch.max(outputs, 1)\n",
    "                val_correct += (val_predicted == y_val).sum().item()\n",
    "                val_total += y_val.size(0)\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "        val_epoch_accuracy = val_correct / val_total\n",
    "        train_history['validation_loss'].append(val_epoch_loss)\n",
    "        train_history['validation_accuracy'].append(val_epoch_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}')\n",
    "    \n",
    "    return train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search 1/5\n",
      "Epoch 1/3, Loss: 0.9893, Accuracy: 0.6288, Validation Loss: 0.9397, Validation Accuracy: 0.6394\n",
      "Epoch 2/3, Loss: 0.9228, Accuracy: 0.6429, Validation Loss: 0.9060, Validation Accuracy: 0.6475\n",
      "Epoch 3/3, Loss: 0.9006, Accuracy: 0.6483, Validation Loss: 0.8915, Validation Accuracy: 0.6507\n",
      "Random Search 2/5\n",
      "Epoch 1/3, Loss: 0.9601, Accuracy: 0.6364, Validation Loss: 0.9047, Validation Accuracy: 0.6487\n",
      "Epoch 2/3, Loss: 0.8835, Accuracy: 0.6542, Validation Loss: 0.8649, Validation Accuracy: 0.6583\n",
      "Epoch 3/3, Loss: 0.8542, Accuracy: 0.6620, Validation Loss: 0.8455, Validation Accuracy: 0.6642\n",
      "Random Search 3/5\n",
      "Epoch 1/3, Loss: 1.0160, Accuracy: 0.6250, Validation Loss: 0.9761, Validation Accuracy: 0.6338\n",
      "Epoch 2/3, Loss: 0.9649, Accuracy: 0.6353, Validation Loss: 0.9524, Validation Accuracy: 0.6377\n",
      "Epoch 3/3, Loss: 0.9493, Accuracy: 0.6383, Validation Loss: 0.9409, Validation Accuracy: 0.6401\n",
      "Random Search 4/5\n",
      "Epoch 1/3, Loss: 1.0032, Accuracy: 0.6269, Validation Loss: 0.9594, Validation Accuracy: 0.6359\n",
      "Epoch 2/3, Loss: 0.9427, Accuracy: 0.6390, Validation Loss: 0.9290, Validation Accuracy: 0.6426\n",
      "Epoch 3/3, Loss: 0.9235, Accuracy: 0.6435, Validation Loss: 0.9144, Validation Accuracy: 0.6461\n",
      "Random Search 5/5\n",
      "Epoch 1/3, Loss: 0.9411, Accuracy: 0.6407, Validation Loss: 0.8742, Validation Accuracy: 0.6566\n",
      "Epoch 2/3, Loss: 0.8486, Accuracy: 0.6639, Validation Loss: 0.8320, Validation Accuracy: 0.6681\n",
      "Epoch 3/3, Loss: 0.8123, Accuracy: 0.6752, Validation Loss: 0.7973, Validation Accuracy: 0.6792\n",
      "Best validation accuracy: 0.6791884839830046\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter space\n",
    "hidden_layer_choices = [[64, 32], \n",
    "                        [128, 64, 32], \n",
    "                        [256, 128, 64], \n",
    "                        [512, 256, 128, 64],\n",
    "                        [64, 32, 16], \n",
    "                        [128, 64, 32, 16], \n",
    "                        [256, 128, 64, 16], \n",
    "                        [512, 256, 128, 64, 16],\n",
    "                        [64, 32, 16, 8], \n",
    "                        [128, 64, 32, 16, 8], \n",
    "                        [256, 128, 64, 16, 8], \n",
    "                        [512, 256, 128, 64, 16, 8]]\n",
    "\n",
    "# Number of random searches\n",
    "num_searches = 5\n",
    "\n",
    "# Store the best model and its performance\n",
    "best_model = None\n",
    "best_val_accuracy = 0\n",
    "best_hidden_dims = None\n",
    "\n",
    "for i in range(num_searches):\n",
    "    print(f\"Random Search {i+1}/{num_searches}\")\n",
    "    hidden_dims = random.choice(hidden_layer_choices)\n",
    "    model = create_model(input_dim, hidden_dims, output_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_history = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "    \n",
    "    # Get the validation accuracy of the final epoch\n",
    "    val_accuracy = train_history['validation_accuracy'][-1]\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "        best_hidden_dims = hidden_dims\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_accuracy}\")\n",
    "print(f\"Best hidden dimensions: {best_hidden_dims}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
